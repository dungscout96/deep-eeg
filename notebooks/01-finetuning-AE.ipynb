{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd3297da-bd26-46f2-8d72-9f89ae93bcdd",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d87544fa-1ecc-4239-bfb8-b49aee079c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-vmbd7g2v because the default path (/home/dutruong/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from libs.dataloaders import mahnob, deap, dreamer\n",
    "from libs.utils import finetuning\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db5efdb-85e8-4961-9119-ee0abd5143d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from libs.dataloaders import mahnob, deap, dreamer\n",
    "from joblib import Parallel, delayed\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad47e4ad-1193-4bc1-841d-4c4104077917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sessions_subjs():\n",
    "    path = \"/net2/expData/affective_eeg/mahnob_dataset/Sessions\"\n",
    "    sessions = os.listdir(path)\n",
    "    subjs = {}\n",
    "    scores = {}\n",
    "    for session in sessions:\n",
    "        mytree = ET.parse(f'{path}/{session}/session.xml')\n",
    "        myroot = mytree.getroot()\n",
    "        if myroot[0].attrib['id'] not in subjs:\n",
    "            subjs[myroot[0].attrib['id']] = []\n",
    "        if 'feltVlnc' in myroot.attrib:\n",
    "            subjs[myroot[0].attrib['id']].append(session)\n",
    "            scores[session] = int(myroot.attrib['feltVlnc'])\n",
    "    return subjs, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "436889f5-0217-40b2-92b9-b0278957db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_cv(dataset, subject_sessions:list, models:list, sweep_pc=False, checkpoint=None):\n",
    "    \"\"\"\n",
    "    Run leave one subject out cross-validation using `model`\n",
    "    Notes:\n",
    "        - Does _not_ enforce train data balancing, but sets equal priors (assumes data is roughly balanced)\n",
    "        - Enforce test data; skip fold when per-label count < 3\n",
    "    @param dataset - dataset\n",
    "    @param list[list] subject_sessions - sessions binned by subject for leave one subject out\n",
    "    @param list models - model to use for training; this should be the class, not the instance\n",
    "    @param bool sweep_pc - sweep the number of PCs\n",
    "    \"\"\"\n",
    "    n_pc_range = np.arange(1, min(len(dataset), len(dataset[0][0][0])+1)) if sweep_pc else np.array([30,]) # min of S and F if sweep enabled\n",
    "    accs = np.empty((len(subject_sessions), len(models), len(n_pc_range))) # n_subjects -> n_models -> n_pc\n",
    "    accs[:] = np.nan\n",
    "    train_accs = accs.copy()\n",
    "    evs = np.empty((len(subject_sessions), len(models), len(n_pc_range)))\n",
    "    evs[:] = np.nan\n",
    "    \n",
    "    for n_subject, sessions in enumerate(tqdm(subject_sessions.values())):\n",
    "        train_data = [dataset[i] for i in range(len(dataset)) if dataset.sessions[i] not in sessions]\n",
    "        test_data = [dataset[i] for i in range(len(dataset)) if dataset.sessions[i] in sessions]\n",
    "        \n",
    "        target_model = finetuning.Finetuning(model=\"AETransformer\", n_freezelayers=0, model_params={'checkpoint': checkpoint})  \n",
    "        input_transform = lambda x, y: target_model._input_augment_fn(x, y)\n",
    "        X_train_vgg = np.array([target_model.model(input_transform(i[0], i[1])[0]).numpy(force=True).flatten() for i in train_data])\n",
    "        Y_train_vgg = np.squeeze([i[1] for i in train_data])*2-1 # rescale to -1, 1\n",
    "        X_test_vgg = np.array([target_model.model(input_transform(i[0], i[1])[0]).numpy(force=True).flatten() for i in test_data])\n",
    "        Y_test_vgg = np.squeeze([i[1] for i in test_data])*2-1 # rescale to -1, 1\n",
    "\n",
    "        if len(Y_train_vgg) == 0 or len(Y_test_vgg) == 0:\n",
    "            print(\"Skipping: Y train or Y test having no values\")\n",
    "            continue\n",
    "        if len(np.unique(Y_test_vgg, return_counts=True)[-1]) != len(np.unique(Y_train_vgg, return_counts=True)[-1]):\n",
    "            print(\"Skipping: num classes not being equivalent in train and test\")\n",
    "            continue\n",
    "\n",
    "        # # This really only works for 2 classes; balance\n",
    "        # u_val, u_count = np.unique(Y_test_vgg, return_counts=True)\n",
    "        # u_count = list(u_count)\n",
    "        # if len(u_count) != 2:\n",
    "        #     print(\"Skipping: class count is not exactly 2\")\n",
    "        #     continue\n",
    "        # if u_count[0] != u_count[1]:\n",
    "        #     # balance the classes to the smaller value\n",
    "        #     u_reduce = u_val[u_count.index(max(u_count))]\n",
    "        #     u_reduce_count = abs(u_count[0] - u_count[1])\n",
    "        #     match_idxs = np.argwhere(Y_test_vgg == u_reduce).squeeze()\n",
    "        #     np.random.shuffle(match_idxs)\n",
    "        #     remove_idxs = match_idxs[:u_reduce_count]\n",
    "        #     keep_idxs = [i for i in range(len(Y_test_vgg)) if i not in remove_idxs]\n",
    "        #     X_test_vgg = X_test_vgg[keep_idxs]\n",
    "        #     Y_test_vgg = Y_test_vgg[keep_idxs]\n",
    "        \n",
    "        if min(np.unique(Y_test_vgg, return_counts=True)[-1]) < 3:\n",
    "            print(\"Skipping: fewer than 3 samples per class detected\")\n",
    "            continue\n",
    "\n",
    "        # Shuffle train\n",
    "        shuffle = np.arange(X_train_vgg.shape[0])\n",
    "        np.random.shuffle(shuffle)\n",
    "        X_train_vgg = X_train_vgg[shuffle]\n",
    "        Y_train_vgg = Y_train_vgg[shuffle]\n",
    "        \n",
    "        for n_model, model in enumerate(models):\n",
    "            pca = PCA() # compute full pc\n",
    "            pca.fit(X_train_vgg)\n",
    "            X_train_pc = pca.transform(X_train_vgg)\n",
    "            X_test_pc = pca.transform(X_test_vgg)\n",
    "            ev_idxs = [i for i in (n_pc_range-1) if i < len(X_train_pc)]\n",
    "            evs[n_subject, n_model, :len(ev_idxs)] = pca.explained_variance_ratio_[ev_idxs]\n",
    "            for n_npc, npc in enumerate(n_pc_range):\n",
    "                model_inst = model()\n",
    "                model_inst.fit(X_train_pc[:,:npc], Y_train_vgg)\n",
    "                model_train = model_inst.score(X_train_pc[:,:npc], Y_train_vgg)\n",
    "                model_test = model_inst.score(X_test_pc[:,:npc], Y_test_vgg)\n",
    "                \n",
    "                accs[n_subject, n_model, n_npc] = model_test\n",
    "                train_accs[n_subject, n_model, n_npc] = model_train\n",
    "    return accs, evs, train_accs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed13b497-b859-442b-b3aa-d8c467847582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_subject_cv(dataset, subject_sessions:list, model:list, n_jobs=12, checkpoint=None):\n",
    "    \"\"\"\n",
    "    Run per subject validation using `model`\n",
    "    Notes:\n",
    "        - Does _not_ enforce train data balancing, but sets equal priors (assumes data is roughly balanced)\n",
    "        - Enforce test data; skip fold when per-label count < 3\n",
    "    @param dataset - DEAP dataset\n",
    "    @param list subject_sessions - subjects or sessions; used for LOO\n",
    "    @param fn model - model to use for training; this should be the class, not the instance\n",
    "    @param int n_jobs - number of parallel jobs to run\n",
    "    \"\"\"\n",
    "    dataset_seqs = len(dataset)\n",
    "    dataset_feats = np.prod(np.shape(dataset[0][0][0]))\n",
    "    pc_sweep_list = np.arange(1, min(dataset_seqs, dataset_feats+1))\n",
    "    \n",
    "    def _thread_worker(session, k_bootstrap, npc, X_train_pc, Y_train_vgg, X_test_pc, Y_test_vgg):\n",
    "        def default_return(msg=\"\"):\n",
    "            return (session, k_bootstrap, npc), msg\n",
    "\n",
    "        if len(Y_train_vgg) == 0 or len(Y_test_vgg) == 0:\n",
    "            return default_return(\"Skipping: Y train or Y test having no values\")\n",
    "        \n",
    "        unique_info = np.unique(Y_train_vgg, return_counts=True)\n",
    "        if len(unique_info[0]) != 2:\n",
    "            return default_return(f\"Skipping: exactly 2 classes not detected\")\n",
    "        if min(unique_info[-1]) < 3:\n",
    "            return default_return(\"Skipping: fewer than 3 samples per class detected\")\n",
    "\n",
    "        model_inst = model()\n",
    "        model_inst.fit(X_train_pc[:,:npc], Y_train_vgg)\n",
    "        model_train = model_inst.score(X_train_pc[:,:npc], Y_train_vgg)\n",
    "        model_test = model_inst.score(X_test_pc[:,:npc], Y_test_vgg)\n",
    "\n",
    "        return (session, k_bootstrap, npc), (pca.explained_variance_ratio_, model_train, model_test)\n",
    "\n",
    "    # Generate parameter set of parallelization\n",
    "    results = []\n",
    "    for session in tqdm(subject_sessions):\n",
    "        data_idx = [i for i in range(len(dataset)) if dataset.sessions[i] == session]\n",
    "        job_list = []\n",
    "        for k_bootstrap, test_idx in enumerate(tqdm(data_idx)):\n",
    "            train_idx = [i for i in data_idx if i != test_idx]\n",
    "            train_data = [dataset[i] for i in train_idx]\n",
    "            \n",
    "            target_model = finetuning.Finetuning(model=\"AETransformer\", n_freezelayers=0, model_params={'checkpoint': checkpoint}) # changed  \n",
    "            input_transform = lambda x, y: target_model._input_augment_fn(x, y)\n",
    "            X_train_vgg = np.array([target_model.model(input_transform(i[0], i[1])[0].unsqueeze(0)).numpy(force=True).flatten() for i in train_data]) # changed\n",
    "            Y_train_vgg = np.squeeze([i[1] for i in train_data])*2-1 # rescale to -1, 1\n",
    "            X_test_vgg = np.array([target_model.model(input_transform(dataset[test_idx][0], dataset[test_idx][1])[0].unsqueeze(0)).numpy(force=True).flatten(),])\n",
    "            Y_test_vgg = np.squeeze(dataset[test_idx][1])*2-1 # rescale to -1, 1\n",
    "            if len(np.shape(Y_train_vgg)) == 0:\n",
    "                Y_train_vgg = np.expand_dims(Y_train_vgg, axis=0)\n",
    "            if len(np.shape(Y_test_vgg)) == 0:\n",
    "                Y_test_vgg = np.expand_dims(Y_test_vgg, axis=0)\n",
    "            \n",
    "            # Shuffle train\n",
    "            shuffle = np.random.permutation(np.arange(X_train_vgg.shape[0]))\n",
    "            X_train_vgg = X_train_vgg[shuffle]\n",
    "            Y_train_vgg = Y_train_vgg[shuffle]\n",
    "\n",
    "            # Fit PC\n",
    "            pca = PCA() # compute full pc\n",
    "            pca.fit(X_train_vgg)\n",
    "            X_train_pc = pca.transform(X_train_vgg)\n",
    "            X_test_pc = pca.transform(X_test_vgg)\n",
    "            \n",
    "            job_list.extend([\n",
    "                (session, k_bootstrap, npc, \n",
    "                    X_train_pc, Y_train_vgg, X_test_pc, Y_test_vgg,\n",
    "                ) for npc in pc_sweep_list])\n",
    "        \n",
    "        # Run model computations\n",
    "        # partial_results = [_thread_worker(*job) for job in tqdm(job_list)]\n",
    "        partial_results = Parallel(n_jobs=n_jobs, backend=\"threading\", verbose=1)(delayed(_thread_worker)(*job) for job in job_list)\n",
    "        results.extend(partial_results)\n",
    "\n",
    "    # Results objects\n",
    "    train_accs = np.empty((len(subject_sessions), len(pc_sweep_list), max(np.unique(dataset.sessions, return_counts=True)[1]))) # n_subjects -> n_pc -> bootstrap\n",
    "    train_accs[:] = np.nan\n",
    "    test_accs = train_accs.copy()\n",
    "    evs = np.empty((len(subject_sessions), max(pc_sweep_list))) # n_subjects -> max(pc_list)\n",
    "    evs[:] = np.nan\n",
    "\n",
    "    # Consolidate results\n",
    "    for n_session, session in enumerate(subject_sessions):\n",
    "        session_result = [i for i in results if i[0][0] == session]\n",
    "        max_npc = max(pc_sweep_list)\n",
    "        for n_npc, npc in enumerate(pc_sweep_list):\n",
    "            npc_result = [i for i in session_result if i[0][2] == npc]\n",
    "            for k_bootstrap in npc_result:\n",
    "                if type(k_bootstrap[1]) is not str:\n",
    "                    train_accs[n_session, n_npc, k_bootstrap[0][1]] = k_bootstrap[1][1]\n",
    "                    test_accs[n_session, n_npc, k_bootstrap[0][1]] = k_bootstrap[1][2]\n",
    "                    if npc == max_npc:\n",
    "                        # across bootstraps values should be near identical\n",
    "                        evs[n_session, :len(k_bootstrap[1][0])] = k_bootstrap[1][0]\n",
    "    \n",
    "    return train_accs, test_accs, evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b97e83b0-2dd7-4a51-90c9-f8ab2413deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainmax2test(train, test):\n",
    "    '''subject pc bootstrap -> subject bootstrap'''\n",
    "    ## Max with fewest nPC\n",
    "    # _train = np.nanmean(train, axis=2) # avg on bootstrap\n",
    "    # _train = np.nan_to_num(_train, nan=-1.) # hacky way to get around all nan argmax\n",
    "    # _argmax = np.argmax(_train, axis=1) # find max acc per pc\n",
    "    # return np.array([subj[_argmax[n_subj], :] for n_subj, subj in enumerate(test)])\n",
    "\n",
    "    ## Max with most nPC\n",
    "    # _train = np.nanmean(train, axis=2) # avg on bootstrap\n",
    "    # _train = np.nan_to_num(_train, nan=-1.) # hacky way to get around all nan argmax\n",
    "    # _train = np.flip(_train, axis=1)\n",
    "    # _argmax = np.argmax(_train, axis=1) # find max acc per pc\n",
    "    # _test = np.flip(test, axis=1)\n",
    "    # _test = np.array([subj[_argmax[n_subj], :] for n_subj, subj in enumerate(_test)])\n",
    "    # _test = np.flip(_test, axis=1)\n",
    "    # return _test\n",
    "\n",
    "    ## Max with most nPC\n",
    "    _train = np.nan_to_num(train, nan=-1.) # hacky way to get around all nan argmax\n",
    "    _train = np.flip(_train, axis=1)\n",
    "    _argmax = np.argmax(_train, axis=1) # find max acc per pc\n",
    "    _test = np.flip(test, axis=1)\n",
    "    return np.array([[k[_argmax[n_subj, n_k]] for n_k, k in enumerate(subj.T)] for n_subj, subj in enumerate(_test)])\n",
    "\n",
    "def plot_overview(train, test, ev, dataset=None, dataset_clip=10):\n",
    "    mdl_accs = trainmax2test(train, test)\n",
    "\n",
    "    if dataset is not None:\n",
    "        print(f\"Clipping datasets with fewer than {dataset_clip} samples...\")\n",
    "        valid_sessions_idx = np.argwhere(np.unique(dataset.sessions, return_counts=True)[1] >= 10).squeeze()\n",
    "    else:\n",
    "        valid_sessions_idx = np.arange(mdl_accs.shape[0])\n",
    "    \n",
    "    mdl_accs = mdl_accs[valid_sessions_idx,:]\n",
    "    mdl_means = np.nanmean(mdl_accs, axis=1) # avg on bootstrap\n",
    "    print(np.nanmean(mdl_means))\n",
    "    mdl_stderr = np.nanstd(mdl_accs, axis=1) / np.sqrt(np.count_nonzero(~np.isnan(mdl_accs), axis=1))\n",
    "    \n",
    "    plt.subplot(2,1,1)\n",
    "    plt.errorbar(np.arange(len(mdl_means)), mdl_means, yerr=mdl_stderr, color='k', ecolor=\"grey\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(np.cumsum(ev, axis=-1).T)\n",
    "    plt.xlabel(\"Subject\")\n",
    "    plt.ylabel(\"Exp Var\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56e59aba-28dc-45d9-83b5-7c54046307d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as torchmodels\n",
    "def vgg16_augment(model):\n",
    "    model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:-3])\n",
    "def inception_augment(model):\n",
    "    model = torch.nn.Sequential(*list(torchmodels.inception_v3(weights=\"DEFAULT\").children())[:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db83db3a-4893-4f14-9e95-60c1af74cca7",
   "metadata": {},
   "source": [
    "# Mahnob Topomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda84ea-b286-4c26-92a4-9002690cbb39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('mahnob.pkl', 'rb') as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "target_model = finetuning.Finetuning(model=\"AE\", n_freezelayers=0, model_params={'checkpoint': '/home/jovyan/deep-eeg/notebooks/AE_mahnob_topomap_adamax_epoch500_seed9'})  \n",
    "sample, label = target_model._input_augment_fn(dataset[18][0], dataset[0][1])\n",
    "recons = target_model.model(sample)\n",
    "# recons -= recons.min(1, keepdim=True)[0]\n",
    "# recons /= recons.max(1, keepdim=True)[0]\n",
    "recons = torch.squeeze(recons).transpose(0,2)\n",
    "# recons.shape\n",
    "print(recons.max())\n",
    "plt.imshow(np.uint16(recons.numpy(force=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd785e-4112-49a4-a01a-67d740a2c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = finetuning.Finetuning(model=\"AE\", n_freezelayers=0, model_params={'checkpoint': '/home/jovyan/deep-eeg/notebooks/AE_mahnob_topomap_adamax_epoch350'})  \n",
    "sample, label = target_model._input_augment_fn(dataset[18][0], dataset[0][1])\n",
    "sample = torch.squeeze(sample).transpose(0,2)\n",
    "# sample -= sample.min(1, keepdim=True)[0]\n",
    "# sample /= sample.max(1, keepdim=True)[0]\n",
    "print(sample.max())\n",
    "plt.imshow(np.uint16(sample.numpy(force=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d916104-4f89-4d07-b6f7-9f4ee1fc98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AE\n",
    "with open('mahnob.pkl', 'rb') as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "target_model = finetuning.Finetuning(model=\"AE\", n_freezelayers=0, model_params={'checkpoint': '/home/jovyan/deep-eeg/notebooks/AE_mahnob_topomap_adamax_epoch350'}, seed=4)  \n",
    "sum_params = 0\n",
    "for param in target_model.model.parameters():\n",
    "    sum_params += torch.sum(param)\n",
    "print(sum_params)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adamax(target_model.model.parameters(), lr = lr)\n",
    "target_model.train(dataset, log_dir='./runs/mahnob/topomap/AEadamax/epoch500/seed4', optimizer=optimizer, lr=lr, epochs=150, early_stopping_eps=0.0001, lr_decay_nepoch=100)\n",
    "torch.save(target_model.model.state_dict(), 'AE_mahnob_topomap_adamax_epoch500_seed4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c9e46-c422-4d33-853d-6d9c12a928fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('mahnob.pkl', 'rb') as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "subjects, subject_scores = get_sessions_subjs()\n",
    "acc, evs, train_acc  = subject_cv(dataset, subjects, models, sweep_pc=True, checkpoint='/home/jovyan/deep-eeg/notebooks/AE_mahnob_topomap_adamax_epoch500_seed9')\n",
    "print('train mean', np.mean(train_acc))\n",
    "print('test mean', np.mean(acc))\n",
    "\n",
    "with open(\"result_finetune_mahnob_topomap_AE_unbalanced.pkl\", \"wb\") as fout:\n",
    "    pickle.dump({\"acc\": acc, \n",
    "                 \"train_acc\": train_acc,\n",
    "                 \"ev\": evs, \"models\": models}, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fff94e-7f74-415e-b26e-251dbcf4e288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for n_model, model in enumerate(models):\n",
    "    mdl_acc = acc[:,n_model,:]\n",
    "    mdl_means = np.nanmean(mdl_acc, axis=0)\n",
    "    mdl_stderr = np.nanstd(mdl_acc, axis=0) / np.sqrt(len(mdl_means))\n",
    "    \n",
    "    plt.subplot(2,1,1)\n",
    "    plt.errorbar(np.arange(len(mdl_means)), mdl_means, yerr=mdl_stderr, color='k', ecolor=\"grey\")\n",
    "    plt.title(f\"{str(model.func).split('.')[-1][:-2]}\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(np.cumsum(evs[:,n_model,:], axis=-1).T)\n",
    "    plt.xlabel(\"# PCs\")\n",
    "    plt.ylabel(\"Exp Var\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917dbeb6-6fa2-4599-b2dc-7b54690399e8",
   "metadata": {},
   "source": [
    "# Mahnob Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010edd4e-02f2-4b38-9e90-1ced7010ddd4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-74804.7891, grad_fn=<AddBackward0>)\n",
      "tensor(-74804.7891, grad_fn=<AddBackward0>)\n",
      "tensor(-74804.7969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "epoch 0:\n",
      "loss=0.006520733789945341\n",
      "weights=-141016.921875\n",
      "\n",
      "epoch 1:\n",
      "loss=0.005795251005423265\n",
      "weights=-144579.09375\n",
      "\n",
      "epoch 2:\n",
      "loss=0.005747887796858758\n",
      "weights=-150476.203125\n",
      "\n",
      "epoch 3:\n",
      "loss=0.005714346190227779\n",
      "weights=-157769.734375\n",
      "\n",
      "epoch 4:\n",
      "loss=0.005697883656154377\n",
      "weights=-163548.21875\n",
      "\n",
      "epoch 5:\n",
      "loss=0.005690327546832086\n",
      "weights=-169499.421875\n",
      "\n",
      "epoch 6:\n",
      "loss=0.0056724931863653995\n",
      "weights=-172892.921875\n",
      "\n",
      "epoch 7:\n",
      "loss=0.005597997814266368\n",
      "weights=-176444.265625\n",
      "\n",
      "epoch 8:\n",
      "loss=0.005441905771302191\n",
      "weights=-181036.078125\n",
      "\n",
      "epoch 9:\n",
      "loss=0.005442650844648744\n",
      "weights=-186722.828125\n",
      "\n",
      "epoch 10:\n",
      "loss=0.005430914633295903\n",
      "weights=-191392.125\n",
      "\n",
      "epoch 11:\n",
      "loss=0.005424353787369763\n",
      "weights=-196429.953125\n",
      "\n",
      "epoch 12:\n",
      "loss=0.005417377486909655\n",
      "weights=-201004.6875\n",
      "\n",
      "epoch 13:\n",
      "loss=0.00540910755587516\n",
      "weights=-205798.046875\n",
      "\n",
      "epoch 14:\n",
      "loss=0.00540381236674471\n",
      "weights=-210796.640625\n",
      "\n",
      "epoch 15:\n",
      "loss=0.005393802829209075\n",
      "weights=-215397.78125\n",
      "\n",
      "epoch 16:\n",
      "loss=0.005387718216022195\n",
      "weights=-219499.203125\n",
      "\n",
      "epoch 17:\n",
      "loss=0.0053830304914483365\n",
      "weights=-223579.359375\n",
      "\n",
      "epoch 18:\n",
      "loss=0.005374770773800485\n",
      "weights=-227511.5\n",
      "\n",
      "epoch 19:\n",
      "loss=0.005366684496755514\n",
      "weights=-231768.59375\n",
      "\n",
      "epoch 20:\n",
      "loss=0.005363223533912778\n",
      "weights=-236591.25\n",
      "\n",
      "epoch 21:\n",
      "loss=0.00535575911165168\n",
      "weights=-241412.203125\n",
      "\n",
      "epoch 22:\n",
      "loss=0.005350289672890659\n",
      "weights=-245574.078125\n",
      "\n",
      "epoch 23:\n",
      "loss=0.005338188687649866\n",
      "weights=-249619.28125\n",
      "\n",
      "epoch 24:\n",
      "loss=0.005332608792502809\n",
      "weights=-253344.71875\n",
      "\n",
      "epoch 25:\n",
      "loss=0.005324607919031435\n",
      "weights=-257440.421875\n",
      "\n",
      "epoch 26:\n",
      "loss=0.005317059525457973\n",
      "weights=-262354.34375\n",
      "\n",
      "epoch 27:\n",
      "loss=0.00530970891622469\n",
      "weights=-266985.5\n",
      "\n",
      "epoch 28:\n",
      "loss=0.005299763411819709\n",
      "weights=-270931.6875\n",
      "\n",
      "epoch 29:\n",
      "loss=0.005294385058052763\n",
      "weights=-275185.25\n",
      "\n",
      "epoch 30:\n",
      "loss=0.00527845273932649\n",
      "weights=-279531.0625\n",
      "\n",
      "epoch 31:\n",
      "loss=0.0052714977117086\n",
      "weights=-284612.875\n",
      "\n",
      "epoch 32:\n",
      "loss=0.005256093641028109\n",
      "weights=-289091.625\n",
      "\n",
      "epoch 33:\n",
      "loss=0.005242061566072281\n",
      "weights=-293868.875\n",
      "\n",
      "epoch 34:\n",
      "loss=0.005227449722823245\n",
      "weights=-298618.71875\n",
      "\n",
      "epoch 35:\n",
      "loss=0.005210675162761801\n",
      "weights=-303159.875\n",
      "\n",
      "epoch 37:\n",
      "loss=0.0051811307003119525\n",
      "weights=-313187.8125\n",
      "\n",
      "epoch 38:\n",
      "loss=0.005161657693650046\n",
      "weights=-318125.53125\n",
      "\n",
      "epoch 39:\n",
      "loss=0.005143421054747181\n",
      "weights=-323353.96875\n",
      "\n",
      "epoch 40:\n",
      "loss=0.005126640146522961\n",
      "weights=-328061.28125\n",
      "\n",
      "epoch 41:\n",
      "loss=0.005102938935372301\n",
      "weights=-332650.0\n",
      "\n",
      "epoch 42:\n",
      "loss=0.005082177377489367\n",
      "weights=-337468.625\n",
      "\n",
      "epoch 43:\n",
      "loss=0.005062645211265507\n",
      "weights=-341939.6875\n",
      "\n",
      "epoch 44:\n",
      "loss=0.005042422843292694\n",
      "weights=-347161.1875\n",
      "\n",
      "epoch 45:\n",
      "loss=0.005018266297128955\n",
      "weights=-351716.875\n",
      "\n",
      "epoch 46:\n",
      "loss=0.004995789449436195\n",
      "weights=-356423.90625\n",
      "\n",
      "epoch 47:\n",
      "loss=0.004973513756248385\n",
      "weights=-361597.84375\n",
      "\n",
      "epoch 48:\n",
      "loss=0.004951557260350033\n",
      "weights=-366157.28125\n",
      "\n",
      "epoch 49:\n",
      "loss=0.004925597644871985\n",
      "weights=-370932.21875\n",
      "\n",
      "epoch 50:\n",
      "loss=0.004904529343199248\n",
      "weights=-375931.125\n",
      "\n",
      "epoch 51:\n",
      "loss=0.004879353796672829\n",
      "weights=-380505.1875\n",
      "\n",
      "epoch 52:\n",
      "loss=0.004855402682983138\n",
      "weights=-385481.96875\n",
      "\n",
      "epoch 53:\n",
      "loss=0.004831890553833371\n",
      "weights=-389617.0625\n",
      "\n",
      "epoch 54:\n",
      "loss=0.004805244452489371\n",
      "weights=-394956.71875\n",
      "\n",
      "epoch 55:\n",
      "loss=0.004786484270366003\n",
      "weights=-398962.09375\n",
      "\n",
      "epoch 57:\n",
      "loss=0.004740804494382117\n",
      "weights=-408409.625\n",
      "\n",
      "epoch 58:\n",
      "loss=0.004715363712595644\n",
      "weights=-413065.28125\n",
      "\n",
      "epoch 59:\n",
      "loss=0.004694052970135641\n",
      "weights=-416983.0\n",
      "\n",
      "epoch 60:\n",
      "loss=0.004669321693287165\n",
      "weights=-421787.1875\n",
      "\n",
      "epoch 61:\n",
      "loss=0.004646163482590569\n",
      "weights=-425836.9375\n",
      "\n",
      "epoch 62:\n",
      "loss=0.004621832168218915\n",
      "weights=-430330.8125\n",
      "\n",
      "epoch 63:\n",
      "loss=0.0046003978944035495\n",
      "weights=-434706.28125\n",
      "\n",
      "epoch 64:\n",
      "loss=0.004573663387381744\n",
      "weights=-439223.625\n",
      "\n",
      "epoch 65:\n",
      "loss=0.004556680354524893\n",
      "weights=-443945.625\n",
      "\n",
      "epoch 66:\n",
      "loss=0.004536423509390178\n",
      "weights=-447693.34375\n",
      "\n",
      "epoch 67:\n",
      "loss=0.004511343057130962\n",
      "weights=-451333.84375\n",
      "\n",
      "epoch 68:\n",
      "loss=0.00448802050268936\n",
      "weights=-455208.5\n",
      "\n",
      "epoch 69:\n",
      "loss=0.004463507294081001\n",
      "weights=-459494.15625\n",
      "\n",
      "epoch 70:\n",
      "loss=0.00443801328638625\n",
      "weights=-463921.40625\n",
      "\n",
      "epoch 71:\n",
      "loss=0.0044228225314961464\n",
      "weights=-468098.4375\n",
      "\n",
      "epoch 72:\n",
      "loss=0.004396166531322053\n",
      "weights=-472242.78125\n",
      "\n",
      "epoch 73:\n",
      "loss=0.0043786082733093265\n",
      "weights=-476393.4375\n",
      "\n",
      "epoch 74:\n",
      "loss=0.00435944737730115\n",
      "weights=-480096.21875\n",
      "\n",
      "epoch 75:\n",
      "loss=0.004333118351900743\n",
      "weights=-484304.96875\n",
      "\n",
      "epoch 76:\n",
      "loss=0.004313390057609238\n",
      "weights=-488343.28125\n",
      "\n",
      "epoch 77:\n",
      "loss=0.004292764412850669\n",
      "weights=-492285.25\n",
      "\n",
      "epoch 78:\n",
      "loss=0.004269624114826773\n",
      "weights=-495657.46875\n",
      "\n",
      "epoch 79:\n",
      "loss=0.004248706468103472\n",
      "weights=-499747.21875\n",
      "\n",
      "epoch 80:\n",
      "loss=0.004226874625968813\n",
      "weights=-503344.53125\n",
      "\n",
      "epoch 81:\n",
      "loss=0.004209728031348663\n",
      "weights=-507368.53125\n",
      "\n",
      "epoch 82:\n",
      "loss=0.004197335662085073\n",
      "weights=-510901.09375\n",
      "\n",
      "epoch 83:\n",
      "loss=0.0041612128720082565\n",
      "weights=-514740.34375\n",
      "\n",
      "epoch 84:\n",
      "loss=0.004147103925898784\n",
      "weights=-518567.09375\n",
      "\n",
      "epoch 85:\n",
      "loss=0.004127354911443862\n",
      "weights=-521725.40625\n",
      "\n",
      "epoch 86:\n",
      "loss=0.00410948004608861\n",
      "weights=-525658.0\n",
      "\n",
      "epoch 87:\n",
      "loss=0.004096386847180324\n",
      "weights=-529567.75\n",
      "\n",
      "epoch 88:\n",
      "loss=0.004075528333560949\n",
      "weights=-533409.9375\n",
      "\n",
      "epoch 89:\n",
      "loss=0.0040529446371812185\n",
      "weights=-536664.125\n",
      "\n",
      "epoch 90:\n",
      "loss=0.004038022592372139\n",
      "weights=-540018.0625\n",
      "\n",
      "epoch 91:\n",
      "loss=0.004020513735024842\n",
      "weights=-543617.1875\n",
      "\n",
      "epoch 92:\n",
      "loss=0.004002516707076192\n",
      "weights=-547195.125\n",
      "\n",
      "epoch 93:\n",
      "loss=0.003979097122227717\n",
      "weights=-550623.25\n",
      "\n",
      "epoch 94:\n",
      "loss=0.003966534997082569\n",
      "weights=-554392.875\n",
      "\n",
      "epoch 95:\n",
      "loss=0.003940238753766424\n",
      "weights=-557682.625\n",
      "\n",
      "epoch 96:\n",
      "loss=0.0039231823527987935\n",
      "weights=-560924.75\n",
      "\n",
      "epoch 97:\n",
      "loss=0.003911913267920038\n",
      "weights=-563975.8125\n",
      "\n",
      "epoch 98:\n",
      "loss=0.003889874702747535\n",
      "weights=-567452.875\n",
      "\n",
      "epoch 99:\n",
      "loss=0.0038751782261474865\n",
      "weights=-570912.5\n",
      "\n",
      "epoch 100:\n",
      "loss=0.0038641096119806546\n",
      "weights=-574328.3125\n",
      "\n",
      "Decay learning rate\n",
      "epoch 101:\n",
      "loss=0.0036756888389436887\n",
      "weights=-574407.0625\n",
      "\n",
      "epoch 102:\n",
      "loss=0.0035845284706517828\n",
      "weights=-574472.9375\n",
      "\n",
      "epoch 103:\n",
      "loss=0.0035386537760260955\n",
      "weights=-574435.0625\n",
      "\n",
      "epoch 104:\n",
      "loss=0.003505982901322458\n",
      "weights=-574295.3125\n",
      "\n",
      "epoch 105:\n",
      "loss=0.003488495960041429\n",
      "weights=-574180.125\n",
      "\n",
      "epoch 106:\n",
      "loss=0.003466167412533641\n",
      "weights=-574103.625\n",
      "\n",
      "epoch 107:\n",
      "loss=0.003444368968073354\n",
      "weights=-573971.8125\n",
      "\n",
      "epoch 108:\n",
      "loss=0.0034271479728913896\n",
      "weights=-573822.1875\n",
      "\n",
      "epoch 109:\n",
      "loss=0.0034191705371168528\n",
      "weights=-573700.4375\n",
      "\n",
      "epoch 110:\n",
      "loss=0.003403885937954365\n",
      "weights=-573535.3125\n",
      "\n",
      "epoch 111:\n",
      "loss=0.0033934221972212798\n",
      "weights=-573384.3125\n",
      "\n",
      "epoch 112:\n",
      "loss=0.0033818421587865385\n",
      "weights=-573279.6875\n",
      "\n",
      "epoch 113:\n",
      "loss=0.0033689649932020617\n",
      "weights=-573107.125\n",
      "\n",
      "epoch 114:\n",
      "loss=0.003360084715155847\n",
      "weights=-572947.0\n",
      "\n",
      "epoch 115:\n",
      "loss=0.003349198507041567\n",
      "weights=-572816.9375\n",
      "\n",
      "epoch 116:\n",
      "loss=0.003340873172309137\n",
      "weights=-572669.0\n",
      "\n",
      "epoch 117:\n",
      "loss=0.0033332491969699134\n",
      "weights=-572576.375\n",
      "\n",
      "epoch 118:\n",
      "loss=0.0033218370188460356\n",
      "weights=-572440.6875\n",
      "\n",
      "epoch 119:\n",
      "loss=0.0033157374765700426\n",
      "weights=-572286.6875\n",
      "\n",
      "epoch 120:\n",
      "loss=0.003306920584314738\n",
      "weights=-572088.9375\n",
      "\n",
      "epoch 121:\n",
      "loss=0.003296474247966244\n",
      "weights=-572045.875\n",
      "\n",
      "epoch 122:\n",
      "loss=0.003291456096434044\n",
      "weights=-571857.4375\n",
      "\n",
      "epoch 123:\n",
      "loss=0.003282332283708107\n",
      "weights=-571730.5625\n",
      "\n",
      "epoch 124:\n",
      "loss=0.003274637352642274\n",
      "weights=-571580.5\n",
      "\n",
      "epoch 125:\n",
      "loss=0.0032676312745539378\n",
      "weights=-571409.9375\n",
      "\n",
      "epoch 126:\n",
      "loss=0.0032590197209936725\n",
      "weights=-571376.4375\n",
      "\n",
      "epoch 127:\n",
      "loss=0.0032506559245203707\n",
      "weights=-571188.75\n",
      "\n",
      "epoch 128:\n",
      "loss=0.0032439595432317055\n",
      "weights=-571145.0625\n",
      "\n",
      "epoch 129:\n",
      "loss=0.0032358377571617527\n",
      "weights=-570960.25\n",
      "\n",
      "epoch 130:\n",
      "loss=0.003229216150962983\n",
      "weights=-570802.75\n",
      "\n",
      "epoch 131:\n",
      "loss=0.0032243106050906007\n",
      "weights=-570675.5625\n",
      "\n",
      "epoch 132:\n",
      "loss=0.0032176005370642123\n",
      "weights=-570572.0\n",
      "\n",
      "epoch 133:\n",
      "loss=0.0032093193090398505\n",
      "weights=-570527.8125\n",
      "\n",
      "epoch 134:\n",
      "loss=0.0032026782132699295\n",
      "weights=-570483.625\n",
      "\n",
      "epoch 135:\n",
      "loss=0.003196881689493441\n",
      "weights=-570340.1875\n",
      "\n",
      "epoch 136:\n",
      "loss=0.0031902140169286857\n",
      "weights=-570263.75\n",
      "\n",
      "epoch 137:\n",
      "loss=0.0031838961674083926\n",
      "weights=-570129.4375\n",
      "\n",
      "epoch 138:\n",
      "loss=0.0031785174564580724\n",
      "weights=-569956.75\n",
      "\n",
      "epoch 139:\n",
      "loss=0.0031722722080597334\n",
      "weights=-569924.9375\n",
      "\n",
      "epoch 140:\n",
      "loss=0.003165732242515066\n",
      "weights=-569836.9375\n",
      "\n",
      "epoch 141:\n",
      "loss=0.0031614817547814147\n",
      "weights=-569694.375\n",
      "\n",
      "epoch 142:\n",
      "loss=0.003155314136294159\n",
      "weights=-569633.4375\n",
      "\n",
      "epoch 143:\n",
      "loss=0.003147932678470482\n",
      "weights=-569445.6875\n",
      "\n",
      "epoch 144:\n",
      "loss=0.003141624220374106\n",
      "weights=-569337.9375\n",
      "\n",
      "epoch 145:\n",
      "loss=0.003137163422188035\n",
      "weights=-569254.0\n",
      "\n",
      "epoch 146:\n",
      "loss=0.00313184956721303\n",
      "weights=-569133.75\n",
      "\n",
      "epoch 147:\n",
      "loss=0.0031278405516088534\n",
      "weights=-569043.0\n",
      "\n",
      "epoch 148:\n",
      "loss=0.0031245202490486055\n",
      "weights=-568971.3125\n",
      "\n",
      "epoch 149:\n",
      "loss=0.0031165879477295234\n",
      "weights=-568926.5625\n",
      "\n",
      "epoch 150:\n",
      "loss=0.0031083524547578447\n",
      "weights=-568875.125\n",
      "\n",
      "epoch 151:\n",
      "loss=0.0031080486275246273\n",
      "weights=-568770.6875\n",
      "\n",
      "epoch 152:\n",
      "loss=0.003100177644742559\n",
      "weights=-568714.8125\n",
      "\n",
      "epoch 153:\n",
      "loss=0.0030958488594735926\n",
      "weights=-568637.5625\n",
      "\n",
      "epoch 154:\n",
      "loss=0.00309179566778252\n",
      "weights=-568589.25\n",
      "\n",
      "epoch 155:\n",
      "loss=0.0030848804453853518\n",
      "weights=-568582.5625\n",
      "\n",
      "epoch 156:\n",
      "loss=0.0030790010274322046\n",
      "weights=-568526.0625\n",
      "\n",
      "epoch 157:\n",
      "loss=0.0030774691705403832\n",
      "weights=-568504.6875\n",
      "\n",
      "epoch 158:\n",
      "loss=0.0030673528481110217\n",
      "weights=-568372.6875\n",
      "\n",
      "epoch 159:\n",
      "loss=0.0030620284307973852\n",
      "weights=-568299.5625\n",
      "\n",
      "epoch 160:\n",
      "loss=0.0030607062046806507\n",
      "weights=-568216.75\n",
      "\n",
      "epoch 161:\n",
      "loss=0.003049090472308712\n",
      "weights=-568165.375\n",
      "\n",
      "epoch 162:\n",
      "loss=0.003048387977572139\n",
      "weights=-568157.25\n",
      "\n",
      "epoch 163:\n",
      "loss=0.0030467655216320183\n",
      "weights=-568134.6875\n",
      "\n",
      "epoch 164:\n",
      "loss=0.0030384356137764914\n",
      "weights=-568154.625\n",
      "\n",
      "epoch 165:\n",
      "loss=0.0030364395378538493\n",
      "weights=-568042.4375\n",
      "\n",
      "epoch 166:\n",
      "loss=0.0030289256176205745\n",
      "weights=-568000.125\n",
      "\n",
      "epoch 167:\n",
      "loss=0.0030255373314521604\n",
      "weights=-567935.9375\n",
      "\n",
      "epoch 168:\n",
      "loss=0.003020980390967011\n",
      "weights=-567932.8125\n",
      "\n",
      "epoch 169:\n",
      "loss=0.0030128458315492467\n",
      "weights=-567858.125\n",
      "\n",
      "epoch 170:\n",
      "loss=0.0030101449055376117\n",
      "weights=-567787.875\n",
      "\n",
      "epoch 171:\n",
      "loss=0.0030041616457053507\n",
      "weights=-567704.8125\n",
      "\n",
      "epoch 172:\n",
      "loss=0.003001056744710006\n",
      "weights=-567680.0\n",
      "\n",
      "epoch 173:\n",
      "loss=0.00299676389561618\n",
      "weights=-567620.4375\n",
      "\n",
      "epoch 174:\n",
      "loss=0.0029900014850856604\n",
      "weights=-567610.5\n",
      "\n",
      "epoch 175:\n",
      "loss=0.002987971241266062\n",
      "weights=-567495.0625\n",
      "\n",
      "epoch 176:\n",
      "loss=0.002983079529120446\n",
      "weights=-567480.625\n",
      "\n",
      "epoch 177:\n",
      "loss=0.002982261225832814\n",
      "weights=-567462.5625\n",
      "\n",
      "epoch 178:\n",
      "loss=0.002972024933004639\n",
      "weights=-567446.25\n",
      "\n",
      "epoch 179:\n",
      "loss=0.0029685950512889623\n",
      "weights=-567306.9375\n",
      "\n",
      "epoch 180:\n",
      "loss=0.0029661684797375903\n",
      "weights=-567268.5625\n",
      "\n",
      "epoch 181:\n",
      "loss=0.0029589825290998427\n",
      "weights=-567219.875\n",
      "\n",
      "epoch 182:\n",
      "loss=0.0029573040538716762\n",
      "weights=-567231.75\n",
      "\n",
      "epoch 183:\n",
      "loss=0.002951066631463949\n",
      "weights=-567265.8125\n",
      "\n",
      "epoch 184:\n",
      "loss=0.00294591494597203\n",
      "weights=-567221.8125\n",
      "\n",
      "epoch 185:\n",
      "loss=0.002945325025733598\n",
      "weights=-567149.125\n",
      "\n",
      "epoch 186:\n",
      "loss=0.002935741731433687\n",
      "weights=-567075.5\n",
      "\n",
      "epoch 187:\n",
      "loss=0.0029362668677214345\n",
      "weights=-567111.4375\n",
      "\n",
      "epoch 188:\n",
      "loss=0.0029320194882180807\n",
      "weights=-567068.5625\n",
      "\n",
      "epoch 189:\n",
      "loss=0.002926383269809638\n",
      "weights=-567053.4375\n",
      "\n",
      "epoch 190:\n",
      "loss=0.0029207261927741007\n",
      "weights=-567056.4375\n",
      "\n",
      "epoch 191:\n",
      "loss=0.0029185995605957664\n",
      "weights=-567058.8125\n",
      "\n",
      "epoch 192:\n",
      "loss=0.0029152290262940404\n",
      "weights=-567109.25\n",
      "\n",
      "epoch 193:\n",
      "loss=0.002909647657140864\n",
      "weights=-567097.5625\n",
      "\n",
      "epoch 194:\n",
      "loss=0.0029044361909112957\n",
      "weights=-567052.375\n",
      "\n",
      "epoch 195:\n",
      "loss=0.002900708698716974\n",
      "weights=-567017.1875\n",
      "\n",
      "epoch 196:\n",
      "loss=0.0029046681384447813\n",
      "weights=-567054.0\n",
      "\n",
      "epoch 197:\n",
      "loss=0.0028999626417197475\n",
      "weights=-567085.0\n",
      "\n",
      "epoch 198:\n",
      "loss=0.002890676556618395\n",
      "weights=-567100.9375\n",
      "\n",
      "epoch 199:\n",
      "loss=0.002886367534026221\n",
      "weights=-567081.5625\n",
      "\n",
      "epoch 200:\n",
      "loss=0.0028806470085824417\n",
      "weights=-567123.0\n",
      "\n",
      "Decay learning rate\n",
      "epoch 201:\n",
      "loss=0.0028579854917938283\n",
      "weights=-567120.8125\n",
      "\n",
      "epoch 202:\n",
      "loss=0.0028532631025000505\n",
      "weights=-567106.125\n",
      "\n",
      "epoch 203:\n",
      "loss=0.00284564831191815\n",
      "weights=-567095.875\n",
      "\n",
      "epoch 204:\n",
      "loss=0.0028430005162719146\n",
      "weights=-567082.625\n",
      "\n",
      "epoch 205:\n",
      "loss=0.0028406421005998674\n",
      "weights=-567076.4375\n",
      "\n",
      "epoch 206:\n",
      "loss=0.0028400611149434077\n",
      "weights=-567067.625\n",
      "\n",
      "epoch 207:\n",
      "loss=0.0028348225239438543\n",
      "weights=-567057.5625\n",
      "\n",
      "epoch 208:\n",
      "loss=0.0028357853192243388\n",
      "weights=-567036.8125\n",
      "\n",
      "epoch 209:\n",
      "loss=0.002834159050976406\n",
      "weights=-567025.6875\n",
      "\n",
      "epoch 210:\n",
      "loss=0.0028317472097849606\n",
      "weights=-567021.0\n",
      "\n",
      "epoch 211:\n",
      "loss=0.0028274728048762136\n",
      "weights=-567014.4375\n",
      "\n",
      "epoch 212:\n",
      "loss=0.0028353361477557984\n",
      "weights=-567008.1875\n",
      "\n",
      "epoch 213:\n",
      "loss=0.0028286631327272965\n",
      "weights=-566999.875\n",
      "\n",
      "epoch 214:\n",
      "loss=0.0028265155563742448\n",
      "weights=-566984.75\n",
      "\n",
      "epoch 215:\n",
      "loss=0.00282513796271888\n",
      "weights=-566968.0\n",
      "\n",
      "epoch 216:\n",
      "loss=0.00282577855684183\n",
      "weights=-566958.9375\n",
      "\n",
      "epoch 217:\n",
      "loss=0.002824001377295571\n",
      "weights=-566939.9375\n",
      "\n",
      "epoch 218:\n",
      "loss=0.0028215030850866113\n",
      "weights=-566924.75\n",
      "\n",
      "epoch 219:\n",
      "loss=0.0028217915648999013\n",
      "weights=-566916.9375\n",
      "\n",
      "epoch 220:\n",
      "loss=0.0028196513885632157\n",
      "weights=-566921.0\n",
      "\n",
      "epoch 221:\n",
      "loss=0.002822107917069448\n",
      "weights=-566921.4375\n",
      "\n",
      "epoch 222:\n",
      "loss=0.002819900100784978\n",
      "weights=-566921.375\n",
      "\n",
      "epoch 223:\n",
      "loss=0.002816832402716344\n",
      "weights=-566912.9375\n",
      "\n",
      "epoch 224:\n",
      "loss=0.0028161019486060007\n",
      "weights=-566893.5\n",
      "\n",
      "epoch 225:\n",
      "loss=0.0028172769787194528\n",
      "weights=-566887.5625\n",
      "\n",
      "epoch 226:\n",
      "loss=0.002816447075350549\n",
      "weights=-566866.0\n",
      "\n",
      "epoch 227:\n",
      "loss=0.0028147553761061657\n",
      "weights=-566860.5625\n",
      "\n",
      "epoch 228:\n",
      "loss=0.0028130557391594984\n",
      "weights=-566852.1875\n",
      "\n",
      "epoch 229:\n",
      "loss=0.0028131848954915475\n",
      "weights=-566836.8125\n",
      "\n",
      "epoch 230:\n",
      "loss=0.002812290004739124\n",
      "weights=-566817.875\n",
      "\n",
      "epoch 231:\n",
      "loss=0.002809344343735244\n",
      "weights=-566810.9375\n",
      "\n",
      "epoch 232:\n",
      "loss=0.0028113608508408446\n",
      "weights=-566812.375\n",
      "\n",
      "epoch 233:\n",
      "loss=0.0028098059398935864\n",
      "weights=-566800.75\n",
      "\n",
      "epoch 234:\n",
      "loss=0.002809565939124429\n",
      "weights=-566787.8125\n",
      "\n",
      "epoch 235:\n",
      "loss=0.00281266079871031\n",
      "weights=-566780.5625\n",
      "\n",
      "epoch 236:\n",
      "loss=0.0028093319663759133\n",
      "weights=-566763.9375\n",
      "\n",
      "epoch 237:\n",
      "loss=0.002804691394123089\n",
      "weights=-566752.5625\n",
      "\n",
      "epoch 238:\n",
      "loss=0.002806907770167472\n",
      "weights=-566745.25\n",
      "\n",
      "epoch 239:\n",
      "loss=0.002806126172101652\n",
      "weights=-566730.5625\n",
      "\n",
      "epoch 240:\n",
      "loss=0.0028045411645599423\n",
      "weights=-566726.0625\n",
      "\n",
      "epoch 241:\n",
      "loss=0.0028085811046008585\n",
      "weights=-566719.8125\n",
      "\n",
      "epoch 242:\n",
      "loss=0.0028058911333885984\n",
      "weights=-566699.8125\n",
      "\n",
      "epoch 243:\n",
      "loss=0.0028064674677237907\n",
      "weights=-566694.125\n",
      "\n",
      "epoch 244:\n",
      "loss=0.0028017942148091442\n",
      "weights=-566684.25\n",
      "\n",
      "epoch 245:\n",
      "loss=0.0028021625443620402\n",
      "weights=-566665.375\n",
      "\n",
      "epoch 246:\n",
      "loss=0.002802452913278507\n",
      "weights=-566653.5\n",
      "\n",
      "epoch 247:\n",
      "loss=0.0028023050561185097\n",
      "weights=-566647.4375\n",
      "\n",
      "epoch 248:\n",
      "loss=0.0028020202116382244\n",
      "weights=-566643.0\n",
      "\n",
      "epoch 249:\n",
      "loss=0.002796272847465136\n",
      "weights=-566635.5\n",
      "\n",
      "epoch 250:\n",
      "loss=0.0027957419777227886\n",
      "weights=-566627.75\n",
      "\n",
      "epoch 251:\n",
      "loss=0.002799595920474861\n",
      "weights=-566614.875\n",
      "\n",
      "epoch 252:\n",
      "loss=0.002800986516486966\n",
      "weights=-566609.375\n",
      "\n",
      "epoch 253:\n",
      "loss=0.0027994232027640925\n",
      "weights=-566613.875\n",
      "\n",
      "epoch 254:\n",
      "loss=0.0027983209719138237\n",
      "weights=-566611.4375\n",
      "\n",
      "epoch 255:\n",
      "loss=0.002800872076697873\n",
      "weights=-566594.625\n",
      "\n",
      "epoch 256:\n",
      "loss=0.0027920030344940833\n",
      "weights=-566590.5\n",
      "\n",
      "epoch 257:\n",
      "loss=0.00279571762487217\n",
      "weights=-566572.1875\n",
      "\n",
      "epoch 258:\n",
      "loss=0.002795697080801627\n",
      "weights=-566563.5625\n",
      "\n",
      "epoch 259:\n",
      "loss=0.002792477993426299\n",
      "weights=-566553.6875\n",
      "\n",
      "epoch 260:\n",
      "loss=0.002790925699471957\n",
      "weights=-566547.3125\n",
      "\n",
      "epoch 261:\n",
      "loss=0.002790849707606766\n",
      "weights=-566541.375\n",
      "\n",
      "epoch 262:\n",
      "loss=0.002792029827950767\n",
      "weights=-566534.625\n",
      "\n",
      "epoch 263:\n",
      "loss=0.0027932488924536777\n",
      "weights=-566529.9375\n",
      "\n",
      "epoch 264:\n",
      "loss=0.0027908897724101375\n",
      "weights=-566518.6875\n",
      "\n",
      "epoch 265:\n",
      "loss=0.0027902320567095144\n",
      "weights=-566513.3125\n",
      "\n",
      "epoch 266:\n",
      "loss=0.0027894623594645488\n",
      "weights=-566497.5\n",
      "\n",
      "epoch 267:\n",
      "loss=0.002789758282215005\n",
      "weights=-566489.5\n",
      "\n",
      "epoch 268:\n",
      "loss=0.002787988921603884\n",
      "weights=-566493.625\n",
      "\n",
      "epoch 269:\n",
      "loss=0.0027895396342501044\n",
      "weights=-566498.9375\n",
      "\n",
      "epoch 270:\n",
      "loss=0.002787118555973237\n",
      "weights=-566474.4375\n",
      "\n",
      "epoch 271:\n",
      "loss=0.00279061865847031\n",
      "weights=-566471.4375\n",
      "\n",
      "epoch 272:\n",
      "loss=0.0027845236857896765\n",
      "weights=-566463.875\n",
      "\n",
      "epoch 273:\n",
      "loss=0.002787436942765113\n",
      "weights=-566443.625\n",
      "\n",
      "epoch 274:\n",
      "loss=0.0027836466859555756\n",
      "weights=-566442.0\n",
      "\n",
      "epoch 275:\n",
      "loss=0.0027851892019048184\n",
      "weights=-566437.8125\n",
      "\n",
      "epoch 276:\n",
      "loss=0.0027852538817872605\n",
      "weights=-566426.0625\n",
      "\n",
      "epoch 277:\n",
      "loss=0.002783344984477894\n",
      "weights=-566418.125\n",
      "\n",
      "epoch 278:\n",
      "loss=0.002782772742905607\n",
      "weights=-566409.8125\n",
      "\n",
      "epoch 279:\n",
      "loss=0.002782442024614982\n",
      "weights=-566397.625\n",
      "\n",
      "epoch 280:\n",
      "loss=0.002782037738602458\n",
      "weights=-566386.5625\n",
      "\n",
      "epoch 281:\n",
      "loss=0.0027797033585786743\n",
      "weights=-566375.375\n",
      "\n",
      "epoch 282:\n",
      "loss=0.002782738165497178\n",
      "weights=-566367.3125\n",
      "\n",
      "epoch 283:\n",
      "loss=0.0027855041941317418\n",
      "weights=-566354.6875\n",
      "\n",
      "epoch 284:\n",
      "loss=0.002783452197403713\n",
      "weights=-566349.1875\n",
      "\n",
      "epoch 285:\n",
      "loss=0.0027801481724100546\n",
      "weights=-566344.9375\n",
      "\n",
      "epoch 286:\n",
      "loss=0.002778319551402496\n",
      "weights=-566337.4375\n",
      "\n",
      "epoch 287:\n",
      "loss=0.00278133244548849\n",
      "weights=-566323.9375\n",
      "\n",
      "epoch 288:\n",
      "loss=0.0027769660498391875\n",
      "weights=-566318.0625\n",
      "\n",
      "epoch 289:\n",
      "loss=0.002776555924245975\n",
      "weights=-566317.8125\n",
      "\n",
      "epoch 290:\n",
      "loss=0.002775177804369837\n",
      "weights=-566307.9375\n",
      "\n",
      "epoch 291:\n",
      "loss=0.002778118460984769\n",
      "weights=-566292.25\n",
      "\n",
      "epoch 292:\n",
      "loss=0.002773986341175388\n",
      "weights=-566288.375\n",
      "\n",
      "epoch 293:\n",
      "loss=0.0027785699325378493\n",
      "weights=-566279.6875\n",
      "\n",
      "epoch 294:\n",
      "loss=0.0027763265751847865\n",
      "weights=-566276.375\n",
      "\n",
      "epoch 295:\n",
      "loss=0.002778952384210013\n",
      "weights=-566266.4375\n",
      "\n",
      "epoch 296:\n",
      "loss=0.00277245362086993\n",
      "weights=-566247.1875\n",
      "\n",
      "epoch 297:\n",
      "loss=0.0027734236406618635\n",
      "weights=-566244.375\n",
      "\n",
      "epoch 298:\n",
      "loss=0.0027750817061674717\n",
      "weights=-566235.125\n",
      "\n",
      "epoch 299:\n",
      "loss=0.0027722248638806057\n",
      "weights=-566236.9375\n",
      "\n",
      "epoch 300:\n",
      "loss=0.002772200627739287\n",
      "weights=-566227.9375\n",
      "\n",
      "Decay learning rate\n",
      "epoch 301:\n",
      "loss=0.002769829623925622\n",
      "weights=-566228.0\n",
      "\n",
      "epoch 302:\n",
      "loss=0.0027674950613414476\n",
      "weights=-566225.3125\n",
      "\n",
      "epoch 303:\n",
      "loss=0.002769538532117043\n",
      "weights=-566223.375\n",
      "\n",
      "epoch 304:\n",
      "loss=0.002771597141000169\n",
      "weights=-566222.375\n",
      "\n",
      "epoch 305:\n",
      "loss=0.0027662473392668133\n",
      "weights=-566221.6875\n",
      "\n",
      "epoch 306:\n",
      "loss=0.002771088548623157\n",
      "weights=-566220.6875\n",
      "\n",
      "epoch 307:\n",
      "loss=0.00276913956945969\n",
      "weights=-566220.0625\n",
      "\n",
      "epoch 308:\n",
      "loss=0.0027685306982179595\n",
      "weights=-566219.25\n",
      "\n",
      "epoch 309:\n",
      "loss=0.0027699248246128923\n",
      "weights=-566217.5625\n",
      "\n",
      "epoch 310:\n",
      "loss=0.002768179656046377\n",
      "weights=-566216.375\n",
      "\n",
      "epoch 311:\n",
      "loss=0.0027694432691064185\n",
      "weights=-566214.6875\n",
      "\n",
      "epoch 312:\n",
      "loss=0.0027703236576259098\n",
      "weights=-566213.125\n",
      "\n",
      "epoch 313:\n",
      "loss=0.002768677634932571\n",
      "weights=-566214.8125\n",
      "\n",
      "epoch 314:\n",
      "loss=0.002769617008097056\n",
      "weights=-566213.5\n",
      "\n",
      "epoch 315:\n",
      "loss=0.0027671543877769373\n",
      "weights=-566212.8125\n",
      "\n",
      "epoch 316:\n",
      "loss=0.002766236128412526\n",
      "weights=-566214.375\n",
      "\n",
      "epoch 317:\n",
      "loss=0.002767115153461657\n",
      "weights=-566213.125\n",
      "\n",
      "epoch 318:\n",
      "loss=0.0027669183780542677\n",
      "weights=-566212.6875\n",
      "\n",
      "epoch 319:\n",
      "loss=0.002768217861438358\n",
      "weights=-566210.375\n",
      "\n",
      "epoch 320:\n",
      "loss=0.0027699051527753284\n",
      "weights=-566209.875\n",
      "\n",
      "epoch 321:\n",
      "loss=0.002767667523879238\n",
      "weights=-566208.5625\n",
      "\n",
      "epoch 322:\n",
      "loss=0.0027660109476666107\n",
      "weights=-566207.6875\n",
      "\n",
      "epoch 323:\n",
      "loss=0.0027683244349618412\n",
      "weights=-566206.125\n",
      "\n",
      "epoch 324:\n",
      "loss=0.0027680675557348877\n",
      "weights=-566204.75\n",
      "\n",
      "epoch 325:\n",
      "loss=0.002767353523828353\n",
      "weights=-566203.375\n",
      "\n",
      "epoch 326:\n",
      "loss=0.00276714331509235\n",
      "weights=-566202.0625\n",
      "\n",
      "epoch 327:\n",
      "loss=0.0027660568967353403\n",
      "weights=-566201.3125\n",
      "\n",
      "epoch 328:\n",
      "loss=0.0027682666250532775\n",
      "weights=-566199.1875\n",
      "\n",
      "epoch 329:\n",
      "loss=0.002768678323429247\n",
      "weights=-566198.25\n",
      "\n",
      "epoch 330:\n",
      "loss=0.002768444782534539\n",
      "weights=-566197.6875\n",
      "\n",
      "epoch 331:\n",
      "loss=0.002765827116114323\n",
      "weights=-566196.4375\n",
      "\n",
      "epoch 332:\n",
      "loss=0.002765093971222564\n",
      "weights=-566194.625\n",
      "\n",
      "epoch 333:\n",
      "loss=0.0027673908255316996\n",
      "weights=-566194.4375\n",
      "\n",
      "epoch 334:\n",
      "loss=0.0027667125057890033\n",
      "weights=-566193.875\n",
      "\n",
      "epoch 335:\n",
      "loss=0.0027663473979419456\n",
      "weights=-566191.6875\n",
      "\n",
      "epoch 336:\n",
      "loss=0.002769488760745247\n",
      "weights=-566190.1875\n",
      "\n",
      "epoch 337:\n",
      "loss=0.0027653418373753966\n",
      "weights=-566190.3125\n",
      "\n",
      "epoch 338:\n",
      "loss=0.0027667500320916073\n",
      "weights=-566190.375\n",
      "\n",
      "epoch 339:\n",
      "loss=0.0027653692267256592\n",
      "weights=-566189.1875\n",
      "\n",
      "epoch 340:\n",
      "loss=0.002766118286709028\n",
      "weights=-566187.6875\n",
      "\n",
      "epoch 341:\n",
      "loss=0.0027667709562758155\n",
      "weights=-566186.0625\n",
      "\n",
      "epoch 342:\n",
      "loss=0.0027632904941606515\n",
      "weights=-566184.6875\n",
      "\n",
      "epoch 343:\n",
      "loss=0.0027619580017412175\n",
      "weights=-566184.3125\n",
      "\n",
      "epoch 344:\n",
      "loss=0.002765492783363137\n",
      "weights=-566183.6875\n",
      "\n",
      "epoch 345:\n",
      "loss=0.0027669557852957475\n",
      "weights=-566182.75\n",
      "\n",
      "epoch 346:\n",
      "loss=0.0027652334050284793\n",
      "weights=-566180.875\n",
      "\n",
      "epoch 347:\n",
      "loss=0.0027652170151622607\n",
      "weights=-566181.0625\n",
      "\n",
      "epoch 348:\n",
      "loss=0.002764146644187\n",
      "weights=-566180.3125\n",
      "\n",
      "epoch 349:\n",
      "loss=0.0027663660573189835\n",
      "weights=-566180.25\n",
      "\n",
      "epoch 350:\n",
      "loss=0.002763467888474803\n",
      "weights=-566179.5625\n",
      "\n",
      "epoch 351:\n",
      "loss=0.0027670170894278346\n",
      "weights=-566177.75\n",
      "\n",
      "epoch 352:\n",
      "loss=0.0027642008849076286\n",
      "weights=-566177.25\n",
      "\n",
      "epoch 353:\n",
      "loss=0.0027644675541927833\n",
      "weights=-566176.5625\n",
      "\n",
      "epoch 354:\n",
      "loss=0.0027652506583083344\n",
      "weights=-566175.5\n",
      "\n",
      "epoch 355:\n",
      "loss=0.0027640683731097126\n",
      "weights=-566174.0\n",
      "\n",
      "epoch 356:\n",
      "loss=0.00276622649269282\n",
      "weights=-566172.5\n",
      "\n",
      "epoch 357:\n",
      "loss=0.0027651637472151168\n",
      "weights=-566171.9375\n",
      "\n",
      "epoch 358:\n",
      "loss=0.002763704536717874\n",
      "weights=-566170.875\n",
      "\n",
      "epoch 359:\n",
      "loss=0.002764585941519607\n",
      "weights=-566169.3125\n",
      "\n",
      "epoch 360:\n",
      "loss=0.0027621035873325486\n",
      "weights=-566168.5625\n",
      "\n",
      "epoch 361:\n",
      "loss=0.002763338480791491\n",
      "weights=-566167.1875\n",
      "\n",
      "epoch 362:\n",
      "loss=0.0027634678290912296\n",
      "weights=-566166.25\n",
      "\n",
      "epoch 363:\n",
      "loss=0.002763244789975667\n",
      "weights=-566165.1875\n",
      "\n",
      "epoch 364:\n",
      "loss=0.0027667084309587614\n",
      "weights=-566164.0625\n",
      "\n",
      "epoch 365:\n",
      "loss=0.0027620909771425744\n",
      "weights=-566162.75\n",
      "\n",
      "epoch 366:\n",
      "loss=0.0027663820429653344\n",
      "weights=-566162.8125\n",
      "\n",
      "epoch 367:\n",
      "loss=0.002765569818262575\n",
      "weights=-566162.875\n",
      "\n",
      "epoch 369:\n",
      "loss=0.0027657971544558127\n",
      "weights=-566162.125\n",
      "\n",
      "epoch 370:\n",
      "loss=0.002764838657722893\n",
      "weights=-566160.5\n",
      "\n",
      "epoch 371:\n",
      "loss=0.002767945494269952\n",
      "weights=-566161.875\n",
      "\n",
      "epoch 372:\n",
      "loss=0.0027652371335525365\n",
      "weights=-566161.0\n",
      "\n",
      "epoch 373:\n",
      "loss=0.002762795681124459\n",
      "weights=-566159.875\n",
      "\n",
      "epoch 374:\n",
      "loss=0.002766022931977241\n",
      "weights=-566157.6875\n",
      "\n",
      "epoch 375:\n",
      "loss=0.0027622090213969726\n",
      "weights=-566156.625\n",
      "\n",
      "epoch 376:\n",
      "loss=0.002760890522922361\n",
      "weights=-566156.125\n",
      "\n",
      "epoch 377:\n",
      "loss=0.0027652511815893267\n",
      "weights=-566153.6875\n",
      "\n",
      "epoch 378:\n",
      "loss=0.0027663413855021426\n",
      "weights=-566153.1875\n",
      "\n",
      "epoch 379:\n",
      "loss=0.0027641871276151654\n",
      "weights=-566153.3125\n",
      "\n",
      "epoch 380:\n",
      "loss=0.002764577658393072\n",
      "weights=-566152.9375\n",
      "\n",
      "epoch 381:\n",
      "loss=0.002759773447942647\n",
      "weights=-566152.0\n",
      "\n",
      "epoch 382:\n",
      "loss=0.002760882497614607\n",
      "weights=-566150.875\n",
      "\n",
      "epoch 383:\n",
      "loss=0.0027629417367867485\n",
      "weights=-566151.0625\n",
      "\n",
      "epoch 384:\n",
      "loss=0.0027606678459877994\n",
      "weights=-566150.5625\n",
      "\n",
      "epoch 385:\n",
      "loss=0.002763311981312893\n",
      "weights=-566150.6875\n",
      "\n",
      "epoch 386:\n",
      "loss=0.0027615141727364238\n",
      "weights=-566148.8125\n",
      "\n",
      "epoch 387:\n",
      "loss=0.002765376330412115\n",
      "weights=-566147.375\n",
      "\n",
      "epoch 388:\n",
      "loss=0.0027630594212119703\n",
      "weights=-566146.5625\n",
      "\n",
      "epoch 389:\n",
      "loss=0.0027627020867334474\n",
      "weights=-566145.375\n",
      "\n",
      "epoch 390:\n",
      "loss=0.002763771676314957\n",
      "weights=-566144.6875\n",
      "\n",
      "epoch 391:\n",
      "loss=0.002762125731525811\n",
      "weights=-566144.0625\n",
      "\n",
      "epoch 392:\n",
      "loss=0.0027597315969284286\n",
      "weights=-566143.0\n",
      "\n",
      "epoch 393:\n",
      "loss=0.0027644091713209308\n",
      "weights=-566141.875\n",
      "\n",
      "epoch 394:\n",
      "loss=0.0027613316511504843\n",
      "weights=-566141.75\n",
      "\n",
      "epoch 395:\n",
      "loss=0.002765556575137783\n",
      "weights=-566139.6875\n",
      "\n",
      "epoch 396:\n",
      "loss=0.002766530343150306\n",
      "weights=-566139.3125\n",
      "\n",
      "epoch 397:\n",
      "loss=0.002766581570889537\n",
      "weights=-566138.0625\n",
      "\n",
      "epoch 398:\n",
      "loss=0.002763209245379337\n",
      "weights=-566138.75\n",
      "\n",
      "epoch 399:\n",
      "loss=0.0027634083873103138\n",
      "weights=-566136.6875\n",
      "\n",
      "epoch 400:\n",
      "loss=0.002766318194746896\n",
      "weights=-566136.8125\n",
      "\n",
      "Decay learning rate\n",
      "epoch 401:\n",
      "loss=0.002763839652573671\n",
      "weights=-566136.9375\n",
      "\n",
      "epoch 402:\n",
      "loss=0.0027611236269656342\n",
      "weights=-566136.875\n",
      "\n",
      "epoch 403:\n",
      "loss=0.002762507290680507\n",
      "weights=-566136.6875\n",
      "\n",
      "epoch 404:\n",
      "loss=0.0027621414755221755\n",
      "weights=-566136.625\n",
      "\n",
      "epoch 405:\n",
      "loss=0.002758648322079086\n",
      "weights=-566136.5625\n",
      "\n",
      "epoch 406:\n",
      "loss=0.002762272628553381\n",
      "weights=-566136.375\n",
      "\n",
      "epoch 407:\n",
      "loss=0.002762944939090033\n",
      "weights=-566136.3125\n",
      "\n",
      "epoch 408:\n",
      "loss=0.002762783085045433\n",
      "weights=-566136.0\n",
      "\n",
      "epoch 409:\n",
      "loss=0.002760578172676491\n",
      "weights=-566135.9375\n",
      "\n",
      "epoch 410:\n",
      "loss=0.002762332407821139\n",
      "weights=-566135.6875\n",
      "\n",
      "epoch 411:\n",
      "loss=0.0027625992864186905\n",
      "weights=-566135.4375\n",
      "\n",
      "epoch 412:\n",
      "loss=0.0027647562039254062\n",
      "weights=-566135.5625\n",
      "\n",
      "epoch 413:\n",
      "loss=0.0027588227525061804\n",
      "weights=-566135.5625\n",
      "\n",
      "epoch 414:\n",
      "loss=0.002760887154815436\n",
      "weights=-566135.5625\n",
      "\n",
      "epoch 415:\n",
      "loss=0.002764895423715276\n",
      "weights=-566135.375\n",
      "\n",
      "epoch 416:\n",
      "loss=0.00276087661628903\n",
      "weights=-566135.125\n",
      "\n",
      "epoch 417:\n",
      "loss=0.0027650312086651948\n",
      "weights=-566135.1875\n",
      "\n",
      "epoch 418:\n",
      "loss=0.0027643689560007795\n",
      "weights=-566135.0\n",
      "\n",
      "epoch 419:\n",
      "loss=0.0027597433054876146\n",
      "weights=-566134.9375\n",
      "\n",
      "epoch 420:\n",
      "loss=0.002761891830801926\n",
      "weights=-566134.8125\n",
      "\n",
      "epoch 421:\n",
      "loss=0.0027615659119975236\n",
      "weights=-566134.75\n",
      "\n",
      "epoch 422:\n",
      "loss=0.002763007574060913\n",
      "weights=-566134.625\n",
      "\n",
      "epoch 423:\n",
      "loss=0.002757874542654426\n",
      "weights=-566134.5\n",
      "\n",
      "epoch 424:\n",
      "loss=0.002762543608439202\n",
      "weights=-566134.375\n",
      "\n",
      "epoch 425:\n",
      "loss=0.002761705831161258\n",
      "weights=-566134.1875\n",
      "\n",
      "epoch 426:\n",
      "loss=0.002764564830953293\n",
      "weights=-566134.0625\n",
      "\n",
      "epoch 427:\n",
      "loss=0.0027661730204309063\n",
      "weights=-566134.0\n",
      "\n",
      "epoch 428:\n",
      "loss=0.0027593746931278006\n",
      "weights=-566133.875\n",
      "\n",
      "epoch 429:\n",
      "loss=0.0027588823803752246\n",
      "weights=-566133.9375\n",
      "\n",
      "epoch 430:\n",
      "loss=0.002761286281512561\n",
      "weights=-566133.8125\n",
      "\n",
      "epoch 431:\n",
      "loss=0.0027628438291492675\n",
      "weights=-566133.625\n",
      "\n",
      "epoch 432:\n",
      "loss=0.0027619131765507085\n",
      "weights=-566133.4375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 9\n",
    "with open('mahnob_spectrogram.pkl', 'rb') as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "target_model = finetuning.Finetuning(model=\"AE\", n_freezelayers=0, seed=seed)  \n",
    "sum_params = 0\n",
    "for param in target_model.model.parameters():\n",
    "    sum_params += torch.sum(param)\n",
    "print(sum_params)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adamax(target_model.model.parameters(), lr = lr)\n",
    "target_model.train(dataset, log_dir='./runs/mahnob/specto/AEAdamax/epoch500/seed'+str(seed), optimizer=optimizer, lr=lr, epochs=500, early_stopping_eps=0.000001, lr_decay_nepoch=100)\n",
    "torch.save(target_model.model.state_dict(), 'AE_mahnob_specto_adamax_epoch500_seed'+str(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaec3e1-1314-4b50-92b0-35ac61a80d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mahnob_spectrogram.pkl', 'rb') as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "subjects, subject_scores = get_sessions_subjs()\n",
    "acc, evs, train_acc  = subject_cv(dataset, subjects, models, sweep_pc=True, checkpoint='/home/jovyan/deep-eeg/notebooks/checkpoints/mahnob/specto/AEAdamax/seed9/epoch_160')\n",
    "print('train mean', np.mean(train_acc))\n",
    "print('test mean', np.mean(acc))\n",
    "\n",
    "with open(\"result_finetune_mahnob_specto_AE_unbalanced.pkl\", \"wb\") as fout:\n",
    "    pickle.dump({\"acc\": acc, \n",
    "                 \"train_acc\": train_acc,\n",
    "                 \"ev\": evs, \"models\": models}, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a42cd-fa32-4a35-a237-67711f1406e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_model, model in enumerate(models):\n",
    "    mdl_acc = acc[:,n_model,:]\n",
    "    mdl_means = np.nanmean(mdl_acc, axis=0)\n",
    "    mdl_stderr = np.nanstd(mdl_acc, axis=0) / np.sqrt(len(mdl_means))\n",
    "    \n",
    "    plt.subplot(2,1,1)\n",
    "    plt.errorbar(np.arange(len(mdl_means)), mdl_means, yerr=mdl_stderr, color='k', ecolor=\"grey\")\n",
    "    plt.title(f\"{str(model.func).split('.')[-1][:-2]}\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(np.cumsum(evs[:,n_model,:], axis=-1).T)\n",
    "    plt.xlabel(\"# PCs\")\n",
    "    plt.ylabel(\"Exp Var\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b28e0-3efb-4d8b-8c96-e928385b9fb7",
   "metadata": {},
   "source": [
    "# Deap Topomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c2899c-0713-4d35-9b4e-d62ad62c7ed4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-74950.9219, grad_fn=<AddBackward0>)\n",
      "tensor(-74950.9219, grad_fn=<AddBackward0>)\n",
      "tensor(-74950.9297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "epoch 0:\n",
      "loss=32566.117652529763\n",
      "weights=-227994.578125\n",
      "\n",
      "epoch 1:\n",
      "loss=27405.22521391369\n",
      "weights=-235623.453125\n",
      "\n",
      "epoch 2:\n",
      "loss=15846.368815104166\n",
      "weights=-241063.984375\n",
      "\n",
      "epoch 3:\n",
      "loss=8890.379557291666\n",
      "weights=-240758.984375\n",
      "\n",
      "epoch 4:\n",
      "loss=8177.931222098215\n",
      "weights=-240476.046875\n",
      "\n",
      "epoch 5:\n",
      "loss=7969.82666015625\n",
      "weights=-240627.015625\n",
      "\n",
      "epoch 6:\n",
      "loss=7898.639834449405\n",
      "weights=-240384.4375\n",
      "\n",
      "epoch 7:\n",
      "loss=7873.5986095610115\n",
      "weights=-240184.15625\n",
      "\n",
      "epoch 8:\n",
      "loss=7864.340727306548\n",
      "weights=-240084.65625\n",
      "\n",
      "epoch 9:\n",
      "loss=7849.563220796131\n",
      "weights=-240029.984375\n",
      "\n",
      "epoch 10:\n",
      "loss=7843.384777250744\n",
      "weights=-239964.109375\n",
      "\n",
      "epoch 11:\n",
      "loss=7830.336460658482\n",
      "weights=-239984.15625\n",
      "\n",
      "epoch 12:\n",
      "loss=7806.447846912202\n",
      "weights=-240114.109375\n",
      "\n",
      "epoch 13:\n",
      "loss=7792.451543898809\n",
      "weights=-240115.390625\n",
      "\n",
      "epoch 14:\n",
      "loss=7796.429268973215\n",
      "weights=-240194.640625\n",
      "\n",
      "epoch 15:\n",
      "loss=7798.9665294828865\n",
      "weights=-240212.203125\n",
      "\n",
      "epoch 16:\n",
      "loss=7781.066906156994\n",
      "weights=-240292.109375\n",
      "\n",
      "epoch 17:\n",
      "loss=7769.770089285715\n",
      "weights=-240398.25\n",
      "\n",
      "epoch 18:\n",
      "loss=7768.397007533482\n",
      "weights=-240695.828125\n",
      "\n",
      "epoch 19:\n",
      "loss=7755.706845238095\n",
      "weights=-240881.484375\n",
      "\n",
      "epoch 20:\n",
      "loss=7755.67974562872\n",
      "weights=-240962.953125\n",
      "\n",
      "epoch 21:\n",
      "loss=7746.083623976934\n",
      "weights=-241174.03125\n",
      "\n",
      "epoch 22:\n",
      "loss=7737.597528366816\n",
      "weights=-241299.53125\n",
      "\n",
      "epoch 23:\n",
      "loss=7749.881719680059\n",
      "weights=-241491.3125\n",
      "\n",
      "epoch 24:\n",
      "loss=7728.2993396577385\n",
      "weights=-241556.625\n",
      "\n",
      "epoch 25:\n",
      "loss=7736.649704706101\n",
      "weights=-241643.375\n",
      "\n",
      "epoch 26:\n",
      "loss=7729.329020182292\n",
      "weights=-241881.203125\n",
      "\n",
      "epoch 27:\n",
      "loss=7714.34001813616\n",
      "weights=-241938.375\n",
      "\n",
      "epoch 28:\n",
      "loss=7716.118361700149\n",
      "weights=-242172.28125\n",
      "\n",
      "epoch 29:\n",
      "loss=7712.331508091518\n",
      "weights=-242322.640625\n",
      "\n",
      "epoch 30:\n",
      "loss=7717.604480561756\n",
      "weights=-242520.375\n",
      "\n",
      "epoch 31:\n",
      "loss=7706.254185267857\n",
      "weights=-242748.640625\n",
      "\n",
      "epoch 32:\n",
      "loss=7692.943952287947\n",
      "weights=-242951.125\n",
      "\n",
      "epoch 33:\n",
      "loss=7688.695835658482\n",
      "weights=-243046.484375\n",
      "\n",
      "epoch 34:\n",
      "loss=7693.321277436756\n",
      "weights=-243261.953125\n",
      "\n",
      "epoch 35:\n",
      "loss=7690.171258835566\n",
      "weights=-243404.875\n",
      "\n",
      "epoch 36:\n",
      "loss=7675.71990094866\n",
      "weights=-243576.5625\n",
      "\n",
      "epoch 37:\n",
      "loss=7674.850830078125\n",
      "weights=-243590.578125\n",
      "\n",
      "epoch 38:\n",
      "loss=7675.74870954241\n",
      "weights=-243776.34375\n",
      "\n",
      "epoch 39:\n",
      "loss=7689.7693103608635\n",
      "weights=-244014.6875\n",
      "\n",
      "epoch 40:\n",
      "loss=7677.058756510417\n",
      "weights=-244224.65625\n",
      "\n",
      "epoch 41:\n",
      "loss=7664.591680617559\n",
      "weights=-244443.046875\n",
      "\n",
      "epoch 42:\n",
      "loss=7665.148484002976\n",
      "weights=-244570.296875\n",
      "\n",
      "epoch 43:\n",
      "loss=7680.225132533482\n",
      "weights=-244947.125\n",
      "\n",
      "epoch 44:\n",
      "loss=7676.0822986421135\n",
      "weights=-245220.515625\n",
      "\n",
      "epoch 45:\n",
      "loss=7670.143182663691\n",
      "weights=-245379.421875\n",
      "\n",
      "epoch 46:\n",
      "loss=7668.16570172991\n",
      "weights=-245586.34375\n",
      "\n",
      "epoch 47:\n",
      "loss=7664.523681640625\n",
      "weights=-245835.484375\n",
      "\n",
      "epoch 48:\n",
      "loss=7654.64154343378\n",
      "weights=-246018.625\n",
      "\n",
      "epoch 49:\n",
      "loss=7655.52386765253\n",
      "weights=-246166.9375\n",
      "\n",
      "epoch 50:\n",
      "loss=7660.608770461309\n",
      "weights=-246341.78125\n",
      "\n",
      "epoch 51:\n",
      "loss=7649.441313244048\n",
      "weights=-246542.3125\n",
      "\n",
      "epoch 52:\n",
      "loss=7668.867059616816\n",
      "weights=-246674.984375\n",
      "\n",
      "epoch 53:\n",
      "loss=7652.00519670759\n",
      "weights=-246912.3125\n",
      "\n",
      "epoch 54:\n",
      "loss=7653.519449869792\n",
      "weights=-247156.171875\n",
      "\n",
      "epoch 55:\n",
      "loss=7650.936407180059\n",
      "weights=-247414.671875\n",
      "\n",
      "epoch 56:\n",
      "loss=7652.481549944197\n",
      "weights=-247665.8125\n",
      "\n",
      "epoch 57:\n",
      "loss=7662.231352306548\n",
      "weights=-247925.578125\n",
      "\n",
      "epoch 58:\n",
      "loss=7643.284900483631\n",
      "weights=-248077.828125\n",
      "\n",
      "epoch 59:\n",
      "loss=7644.193917410715\n",
      "weights=-248320.25\n",
      "\n",
      "epoch 60:\n",
      "loss=7644.304768880208\n",
      "weights=-248523.546875\n",
      "\n",
      "epoch 61:\n",
      "loss=7649.635288783482\n",
      "weights=-248844.765625\n",
      "\n",
      "epoch 62:\n",
      "loss=7643.68962751116\n",
      "weights=-249104.59375\n",
      "\n",
      "epoch 63:\n",
      "loss=7636.63226609003\n",
      "weights=-249318.828125\n",
      "\n",
      "epoch 64:\n",
      "loss=7650.059198288691\n",
      "weights=-249541.84375\n",
      "\n",
      "epoch 65:\n",
      "loss=7641.61052594866\n",
      "weights=-249828.90625\n",
      "\n",
      "epoch 66:\n",
      "loss=7649.660458519345\n",
      "weights=-250123.609375\n",
      "\n",
      "epoch 67:\n",
      "loss=7638.106189546131\n",
      "weights=-250353.5\n",
      "\n",
      "epoch 68:\n",
      "loss=7631.231317429316\n",
      "weights=-250660.484375\n",
      "\n",
      "epoch 69:\n",
      "loss=7641.083856491816\n",
      "weights=-250844.359375\n",
      "\n",
      "epoch 70:\n",
      "loss=7643.184047154018\n",
      "weights=-251115.265625\n",
      "\n",
      "epoch 71:\n",
      "loss=7646.478876023066\n",
      "weights=-251409.8125\n",
      "\n",
      "epoch 72:\n",
      "loss=7639.681082589285\n",
      "weights=-251705.921875\n",
      "\n",
      "epoch 73:\n",
      "loss=7641.708565848215\n",
      "weights=-251983.78125\n",
      "\n",
      "epoch 74:\n",
      "loss=7632.10564313616\n",
      "weights=-252187.75\n",
      "\n",
      "epoch 75:\n",
      "loss=7638.858410063244\n",
      "weights=-252587.296875\n",
      "\n",
      "epoch 76:\n",
      "loss=7637.2138904389885\n",
      "weights=-252823.375\n",
      "\n",
      "epoch 77:\n",
      "loss=7649.30859375\n",
      "weights=-253127.765625\n",
      "\n",
      "epoch 78:\n",
      "loss=7625.116350446428\n",
      "weights=-253303.859375\n",
      "\n",
      "epoch 79:\n",
      "loss=7628.124918619792\n",
      "weights=-253450.484375\n",
      "\n",
      "epoch 80:\n",
      "loss=7621.6232677641365\n",
      "weights=-253601.015625\n",
      "\n",
      "epoch 81:\n",
      "loss=7630.151471819197\n",
      "weights=-253792.40625\n",
      "\n",
      "epoch 82:\n",
      "loss=7621.302153087798\n",
      "weights=-253911.484375\n",
      "\n",
      "epoch 83:\n",
      "loss=7625.427746000744\n",
      "weights=-254212.953125\n",
      "\n",
      "epoch 84:\n",
      "loss=7632.043340773809\n",
      "weights=-254446.1875\n",
      "\n",
      "epoch 85:\n",
      "loss=7627.0512927827385\n",
      "weights=-254622.484375\n",
      "\n",
      "epoch 86:\n",
      "loss=7626.818777901785\n",
      "weights=-255007.203125\n",
      "\n",
      "epoch 87:\n",
      "loss=7622.177594866072\n",
      "weights=-255189.9375\n",
      "\n",
      "epoch 88:\n",
      "loss=7167.175211588542\n",
      "weights=-255549.5\n",
      "\n",
      "epoch 89:\n",
      "loss=4008.001714797247\n",
      "weights=-256538.765625\n",
      "\n",
      "epoch 90:\n",
      "loss=1620.2833041236513\n",
      "weights=-256870.953125\n",
      "\n",
      "epoch 91:\n",
      "loss=147.10467129661924\n",
      "weights=-257575.9375\n",
      "\n",
      "epoch 92:\n",
      "loss=138.76739647274925\n",
      "weights=-257934.15625\n",
      "\n",
      "epoch 93:\n",
      "loss=138.08793167840867\n",
      "weights=-258430.125\n",
      "\n",
      "epoch 94:\n",
      "loss=123.71256655738468\n",
      "weights=-258807.203125\n",
      "\n",
      "epoch 95:\n",
      "loss=146.3828866141183\n",
      "weights=-259199.265625\n",
      "\n",
      "epoch 96:\n",
      "loss=130.46210570562454\n",
      "weights=-259613.3125\n",
      "\n",
      "epoch 97:\n",
      "loss=127.716491154262\n",
      "weights=-259862.125\n",
      "\n",
      "epoch 98:\n",
      "loss=119.78961926414853\n",
      "weights=-260173.78125\n",
      "\n",
      "epoch 99:\n",
      "loss=116.42548533848354\n",
      "weights=-260370.65625\n",
      "\n",
      "epoch 100:\n",
      "loss=140.08098820277624\n",
      "weights=-260763.9375\n",
      "\n",
      "Decay learning rate\n",
      "epoch 101:\n",
      "loss=108.34050623575847\n",
      "weights=-260777.65625\n",
      "\n",
      "epoch 102:\n",
      "loss=103.91735640026275\n",
      "weights=-260758.46875\n",
      "\n",
      "epoch 103:\n",
      "loss=102.99894732520694\n",
      "weights=-260739.390625\n",
      "\n",
      "epoch 104:\n",
      "loss=102.21278508504231\n",
      "weights=-260718.65625\n",
      "\n",
      "epoch 105:\n",
      "loss=101.72167605445499\n",
      "weights=-260703.59375\n",
      "\n",
      "epoch 106:\n",
      "loss=100.77294967288063\n",
      "weights=-260690.921875\n",
      "\n",
      "epoch 107:\n",
      "loss=100.95884195963542\n",
      "weights=-260693.453125\n",
      "\n",
      "epoch 108:\n",
      "loss=101.0705809820266\n",
      "weights=-260687.65625\n",
      "\n",
      "epoch 109:\n",
      "loss=99.41753659929547\n",
      "weights=-260666.28125\n",
      "\n",
      "epoch 110:\n",
      "loss=99.10808563232422\n",
      "weights=-260648.796875\n",
      "\n",
      "epoch 111:\n",
      "loss=98.92850639706566\n",
      "weights=-260633.625\n",
      "\n",
      "epoch 112:\n",
      "loss=98.53592391241165\n",
      "weights=-260617.0625\n",
      "\n",
      "epoch 113:\n",
      "loss=98.37535567510696\n",
      "weights=-260600.46875\n",
      "\n",
      "epoch 114:\n",
      "loss=97.99866085960751\n",
      "weights=-260594.171875\n",
      "\n",
      "epoch 115:\n",
      "loss=97.59679240272159\n",
      "weights=-260555.21875\n",
      "\n",
      "epoch 116:\n",
      "loss=97.90172794886998\n",
      "weights=-260544.0625\n",
      "\n",
      "epoch 117:\n",
      "loss=97.50397545950753\n",
      "weights=-260530.109375\n",
      "\n",
      "epoch 118:\n",
      "loss=96.92427517118908\n",
      "weights=-260508.203125\n",
      "\n",
      "epoch 119:\n",
      "loss=96.12668954758416\n",
      "weights=-260499.609375\n",
      "\n",
      "epoch 120:\n",
      "loss=96.15853954496838\n",
      "weights=-260476.890625\n",
      "\n",
      "epoch 121:\n",
      "loss=96.13053548903693\n",
      "weights=-260451.8125\n",
      "\n",
      "epoch 122:\n",
      "loss=95.52539453052339\n",
      "weights=-260446.21875\n",
      "\n",
      "epoch 123:\n",
      "loss=96.0172355288551\n",
      "weights=-260421.171875\n",
      "\n",
      "epoch 124:\n",
      "loss=95.60924729846772\n",
      "weights=-260438.1875\n",
      "\n",
      "epoch 125:\n",
      "loss=94.87272335234142\n",
      "weights=-260428.828125\n",
      "\n",
      "epoch 126:\n",
      "loss=95.23900913056873\n",
      "weights=-260408.421875\n",
      "\n",
      "epoch 127:\n",
      "loss=94.40314084007626\n",
      "weights=-260377.703125\n",
      "\n",
      "epoch 128:\n",
      "loss=94.1240832010905\n",
      "weights=-260389.09375\n",
      "\n",
      "epoch 129:\n",
      "loss=93.92114957173665\n",
      "weights=-260395.953125\n",
      "\n",
      "epoch 130:\n",
      "loss=93.94590005420503\n",
      "weights=-260376.65625\n",
      "\n",
      "epoch 131:\n",
      "loss=93.22898265293666\n",
      "weights=-260362.40625\n",
      "\n",
      "epoch 132:\n",
      "loss=93.49173763820103\n",
      "weights=-260380.265625\n",
      "\n",
      "epoch 133:\n",
      "loss=93.1579940432594\n",
      "weights=-260377.625\n",
      "\n",
      "epoch 134:\n",
      "loss=92.32804470970517\n",
      "weights=-260361.90625\n",
      "\n",
      "epoch 135:\n",
      "loss=93.73573266892205\n",
      "weights=-260361.03125\n",
      "\n",
      "epoch 136:\n",
      "loss=91.48307346162342\n",
      "weights=-260356.890625\n",
      "\n",
      "epoch 137:\n",
      "loss=91.47595814296177\n",
      "weights=-260362.28125\n",
      "\n",
      "epoch 138:\n",
      "loss=91.49125589643207\n",
      "weights=-260360.953125\n",
      "\n",
      "epoch 139:\n",
      "loss=90.23342196146648\n",
      "weights=-260346.65625\n",
      "\n",
      "epoch 140:\n",
      "loss=90.84988139924549\n",
      "weights=-260334.203125\n",
      "\n",
      "epoch 141:\n",
      "loss=90.06665366036552\n",
      "weights=-260315.15625\n",
      "\n",
      "epoch 142:\n",
      "loss=91.63374746413459\n",
      "weights=-260323.5625\n",
      "\n",
      "epoch 143:\n",
      "loss=89.87786283947173\n",
      "weights=-260309.859375\n",
      "\n",
      "epoch 144:\n",
      "loss=88.8875059400286\n",
      "weights=-260313.875\n",
      "\n",
      "epoch 145:\n",
      "loss=88.67790540059407\n",
      "weights=-260308.28125\n",
      "\n",
      "epoch 146:\n",
      "loss=88.81591388157436\n",
      "weights=-260293.609375\n",
      "\n",
      "epoch 147:\n",
      "loss=87.3881155649821\n",
      "weights=-260286.65625\n",
      "\n",
      "epoch 148:\n",
      "loss=88.26197905767532\n",
      "weights=-260270.171875\n",
      "\n",
      "epoch 149:\n",
      "loss=87.56415794009254\n",
      "weights=-260265.921875\n",
      "\n",
      "epoch 150:\n",
      "loss=87.0657142457508\n",
      "weights=-260276.671875\n",
      "\n",
      "epoch 151:\n",
      "loss=86.88406871614002\n",
      "weights=-260272.09375\n",
      "\n",
      "epoch 152:\n",
      "loss=87.35789689563569\n",
      "weights=-260265.78125\n",
      "\n",
      "epoch 153:\n",
      "loss=87.12484486897786\n",
      "weights=-260278.546875\n",
      "\n",
      "epoch 154:\n",
      "loss=86.11402675083706\n",
      "weights=-260249.5\n",
      "\n",
      "epoch 155:\n",
      "loss=87.32860447111584\n",
      "weights=-260243.9375\n",
      "\n",
      "epoch 156:\n",
      "loss=86.1638119107201\n",
      "weights=-260258.0625\n",
      "\n",
      "epoch 157:\n",
      "loss=86.41129257565453\n",
      "weights=-260277.515625\n",
      "\n",
      "epoch 158:\n",
      "loss=85.45562062944684\n",
      "weights=-260286.40625\n",
      "\n",
      "epoch 159:\n",
      "loss=84.67552130562919\n",
      "weights=-260280.421875\n",
      "\n",
      "epoch 160:\n",
      "loss=84.574736731393\n",
      "weights=-260287.140625\n",
      "\n",
      "epoch 161:\n",
      "loss=84.71008337111701\n",
      "weights=-260287.96875\n",
      "\n",
      "epoch 162:\n",
      "loss=84.51142783392044\n",
      "weights=-260294.828125\n",
      "\n",
      "epoch 163:\n",
      "loss=83.9306454431443\n",
      "weights=-260296.09375\n",
      "\n",
      "epoch 164:\n",
      "loss=83.06812204633441\n",
      "weights=-260302.171875\n",
      "\n",
      "epoch 165:\n",
      "loss=83.22045853024437\n",
      "weights=-260310.265625\n",
      "\n",
      "epoch 166:\n",
      "loss=82.74937956673759\n",
      "weights=-260337.671875\n",
      "\n",
      "epoch 167:\n",
      "loss=82.22963242303757\n",
      "weights=-260340.171875\n",
      "\n",
      "epoch 168:\n",
      "loss=83.34835997081939\n",
      "weights=-260323.859375\n",
      "\n",
      "epoch 169:\n",
      "loss=82.93505496070499\n",
      "weights=-260356.171875\n",
      "\n",
      "epoch 170:\n",
      "loss=83.20615059988839\n",
      "weights=-260345.5625\n",
      "\n",
      "epoch 171:\n",
      "loss=81.00712885175433\n",
      "weights=-260338.625\n",
      "\n",
      "epoch 172:\n",
      "loss=81.44747479756673\n",
      "weights=-260339.859375\n",
      "\n",
      "epoch 173:\n",
      "loss=81.61802355448405\n",
      "weights=-260374.953125\n",
      "\n",
      "epoch 174:\n",
      "loss=81.04839679173061\n",
      "weights=-260372.0625\n",
      "\n",
      "epoch 175:\n",
      "loss=81.53283055623372\n",
      "weights=-260388.296875\n",
      "\n",
      "epoch 176:\n",
      "loss=81.19523366292317\n",
      "weights=-260404.15625\n",
      "\n",
      "epoch 177:\n",
      "loss=80.96374121166411\n",
      "weights=-260402.3125\n",
      "\n",
      "epoch 178:\n",
      "loss=82.10880552019391\n",
      "weights=-260398.25\n",
      "\n",
      "epoch 179:\n",
      "loss=80.6155332837786\n",
      "weights=-260409.53125\n",
      "\n",
      "epoch 180:\n",
      "loss=79.51087670099167\n",
      "weights=-260432.578125\n",
      "\n",
      "epoch 181:\n",
      "loss=79.75719651721772\n",
      "weights=-260422.5625\n",
      "\n",
      "epoch 182:\n",
      "loss=81.98892575218564\n",
      "weights=-260482.125\n",
      "\n",
      "epoch 183:\n",
      "loss=78.91327285766602\n",
      "weights=-260456.46875\n",
      "\n",
      "epoch 184:\n",
      "loss=79.72164599100749\n",
      "weights=-260476.078125\n",
      "\n",
      "epoch 185:\n",
      "loss=79.35959098452614\n",
      "weights=-260485.296875\n",
      "\n",
      "epoch 186:\n",
      "loss=81.89390818277995\n",
      "weights=-260498.734375\n",
      "\n",
      "epoch 187:\n",
      "loss=78.78943043663388\n",
      "weights=-260495.640625\n",
      "\n",
      "epoch 188:\n",
      "loss=79.22224916730609\n",
      "weights=-260495.703125\n",
      "\n",
      "epoch 189:\n",
      "loss=77.56335703531902\n",
      "weights=-260519.546875\n",
      "\n",
      "epoch 190:\n",
      "loss=76.6164379119873\n",
      "weights=-260536.140625\n",
      "\n",
      "epoch 191:\n",
      "loss=78.28760583060128\n",
      "weights=-260527.546875\n",
      "\n",
      "epoch 192:\n",
      "loss=77.95974359058198\n",
      "weights=-260532.421875\n",
      "\n",
      "epoch 193:\n",
      "loss=77.43232445489792\n",
      "weights=-260563.03125\n",
      "\n",
      "epoch 194:\n",
      "loss=78.44033468337287\n",
      "weights=-260568.203125\n",
      "\n",
      "epoch 195:\n",
      "loss=77.16738709949311\n",
      "weights=-260608.171875\n",
      "\n",
      "epoch 196:\n",
      "loss=76.44809822809128\n",
      "weights=-260601.4375\n",
      "\n",
      "epoch 197:\n",
      "loss=76.13347525823684\n",
      "weights=-260608.46875\n",
      "\n",
      "epoch 198:\n",
      "loss=76.91394506181989\n",
      "weights=-260633.1875\n",
      "\n",
      "epoch 199:\n",
      "loss=75.75356447129022\n",
      "weights=-260630.734375\n",
      "\n",
      "epoch 200:\n",
      "loss=76.13039752415249\n",
      "weights=-260630.84375\n",
      "\n",
      "Decay learning rate\n",
      "epoch 201:\n",
      "loss=74.5299494607108\n",
      "weights=-260631.25\n",
      "\n",
      "epoch 202:\n",
      "loss=73.96432931082589\n",
      "weights=-260634.0\n",
      "\n",
      "epoch 203:\n",
      "loss=74.18133217947823\n",
      "weights=-260631.296875\n",
      "\n",
      "epoch 204:\n",
      "loss=73.95351210094634\n",
      "weights=-260630.0\n",
      "\n",
      "epoch 205:\n",
      "loss=73.83173847198486\n",
      "weights=-260630.140625\n",
      "\n",
      "epoch 206:\n",
      "loss=73.75433504013787\n",
      "weights=-260628.0\n",
      "\n",
      "epoch 207:\n",
      "loss=73.94761367071243\n",
      "weights=-260624.46875\n",
      "\n",
      "epoch 208:\n",
      "loss=73.86531975155785\n",
      "weights=-260620.28125\n",
      "\n",
      "epoch 209:\n",
      "loss=74.0395758492606\n",
      "weights=-260621.53125\n",
      "\n",
      "epoch 210:\n",
      "loss=74.41159266517276\n",
      "weights=-260618.25\n",
      "\n",
      "epoch 211:\n",
      "loss=73.63934071858723\n",
      "weights=-260617.328125\n",
      "\n",
      "epoch 212:\n",
      "loss=73.49904578072685\n",
      "weights=-260618.671875\n",
      "\n",
      "epoch 213:\n",
      "loss=73.78065408979144\n",
      "weights=-260619.578125\n",
      "\n",
      "epoch 214:\n",
      "loss=73.67272077287946\n",
      "weights=-260617.703125\n",
      "\n",
      "epoch 215:\n",
      "loss=73.70746821448917\n",
      "weights=-260617.53125\n",
      "\n",
      "epoch 216:\n",
      "loss=73.49778611319405\n",
      "weights=-260619.65625\n",
      "\n",
      "epoch 217:\n",
      "loss=73.46341387430827\n",
      "weights=-260621.0625\n",
      "\n",
      "epoch 218:\n",
      "loss=73.52049091884068\n",
      "weights=-260618.125\n",
      "\n",
      "epoch 219:\n",
      "loss=73.64651316688175\n",
      "weights=-260613.34375\n",
      "\n",
      "epoch 220:\n",
      "loss=73.90624827430362\n",
      "weights=-260613.4375\n",
      "\n",
      "epoch 221:\n",
      "loss=73.1952398390997\n",
      "weights=-260609.890625\n",
      "\n",
      "epoch 222:\n",
      "loss=73.0087121327718\n",
      "weights=-260608.125\n",
      "\n",
      "epoch 223:\n",
      "loss=73.66494215102423\n",
      "weights=-260609.65625\n",
      "\n",
      "epoch 224:\n",
      "loss=72.89596584865025\n",
      "weights=-260608.71875\n",
      "\n",
      "epoch 225:\n",
      "loss=72.62757819039481\n",
      "weights=-260604.859375\n",
      "\n",
      "epoch 226:\n",
      "loss=72.92471177237374\n",
      "weights=-260602.203125\n",
      "\n",
      "epoch 227:\n",
      "loss=73.42728623889741\n",
      "weights=-260601.171875\n",
      "\n",
      "epoch 228:\n",
      "loss=73.05458586556571\n",
      "weights=-260599.9375\n",
      "\n",
      "epoch 229:\n",
      "loss=73.10507229396275\n",
      "weights=-260601.9375\n",
      "\n",
      "epoch 230:\n",
      "loss=73.21855926513672\n",
      "weights=-260601.484375\n",
      "\n",
      "epoch 231:\n",
      "loss=73.43747620355515\n",
      "weights=-260605.65625\n",
      "\n",
      "epoch 232:\n",
      "loss=73.10825947352818\n",
      "weights=-260603.234375\n",
      "\n",
      "epoch 233:\n",
      "loss=72.69314638773601\n",
      "weights=-260603.03125\n",
      "\n",
      "epoch 234:\n",
      "loss=72.32659730457124\n",
      "weights=-260601.609375\n",
      "\n",
      "epoch 235:\n",
      "loss=72.79622586568196\n",
      "weights=-260599.046875\n",
      "\n",
      "epoch 236:\n",
      "loss=72.69870031447638\n",
      "weights=-260595.015625\n",
      "\n",
      "epoch 237:\n",
      "loss=72.52122542971657\n",
      "weights=-260593.1875\n",
      "\n",
      "epoch 238:\n",
      "loss=72.27959451221284\n",
      "weights=-260596.59375\n",
      "\n",
      "epoch 239:\n",
      "loss=72.54111035664876\n",
      "weights=-260595.5625\n",
      "\n",
      "epoch 240:\n",
      "loss=72.29437428429013\n",
      "weights=-260591.53125\n",
      "\n",
      "epoch 241:\n",
      "loss=72.3611551920573\n",
      "weights=-260584.3125\n",
      "\n",
      "epoch 242:\n",
      "loss=72.58889806838263\n",
      "weights=-260587.03125\n",
      "\n",
      "epoch 243:\n",
      "loss=72.35787691388812\n",
      "weights=-260587.546875\n",
      "\n",
      "epoch 244:\n",
      "loss=72.33498328072685\n",
      "weights=-260585.796875\n",
      "\n",
      "epoch 245:\n",
      "loss=72.1089636484782\n",
      "weights=-260587.15625\n",
      "\n",
      "epoch 246:\n",
      "loss=72.29861768086751\n",
      "weights=-260590.21875\n",
      "\n",
      "epoch 247:\n",
      "loss=72.21068591163272\n",
      "weights=-260589.75\n",
      "\n",
      "epoch 248:\n",
      "loss=72.45053745451428\n",
      "weights=-260591.875\n",
      "\n",
      "epoch 249:\n",
      "loss=72.42456354413714\n",
      "weights=-260586.484375\n",
      "\n",
      "epoch 250:\n",
      "loss=72.0383064633324\n",
      "weights=-260585.890625\n",
      "\n",
      "epoch 251:\n",
      "loss=72.50237882704963\n",
      "weights=-260584.1875\n",
      "\n",
      "epoch 252:\n",
      "loss=72.26484362284343\n",
      "weights=-260586.5625\n",
      "\n",
      "epoch 253:\n",
      "loss=72.12974621000744\n",
      "weights=-260586.203125\n",
      "\n",
      "epoch 254:\n",
      "loss=71.8833243960426\n",
      "weights=-260583.578125\n",
      "\n",
      "epoch 255:\n",
      "loss=71.95547176542736\n",
      "weights=-260583.4375\n",
      "\n",
      "epoch 256:\n",
      "loss=71.54568844749814\n",
      "weights=-260585.296875\n",
      "\n",
      "epoch 257:\n",
      "loss=72.05023683820453\n",
      "weights=-260586.640625\n",
      "\n",
      "epoch 258:\n",
      "loss=72.00371769496373\n",
      "weights=-260587.171875\n",
      "\n",
      "epoch 259:\n",
      "loss=71.3743688492548\n",
      "weights=-260582.25\n",
      "\n",
      "epoch 260:\n",
      "loss=72.49883851550874\n",
      "weights=-260586.890625\n",
      "\n",
      "epoch 261:\n",
      "loss=71.21794800531296\n",
      "weights=-260584.953125\n",
      "\n",
      "epoch 262:\n",
      "loss=71.92116819109235\n",
      "weights=-260588.96875\n",
      "\n",
      "epoch 263:\n",
      "loss=71.3822482881092\n",
      "weights=-260588.375\n",
      "\n",
      "epoch 264:\n",
      "loss=71.46386191958473\n",
      "weights=-260586.140625\n",
      "\n",
      "epoch 265:\n",
      "loss=71.55738430931454\n",
      "weights=-260584.59375\n",
      "\n",
      "epoch 266:\n",
      "loss=71.36945179530552\n",
      "weights=-260584.078125\n",
      "\n",
      "epoch 267:\n",
      "loss=71.60767691476005\n",
      "weights=-260583.53125\n",
      "\n",
      "epoch 268:\n",
      "loss=71.57551502046131\n",
      "weights=-260581.34375\n",
      "\n",
      "epoch 269:\n",
      "loss=70.87758164178757\n",
      "weights=-260582.984375\n",
      "\n",
      "epoch 270:\n",
      "loss=71.18968536740257\n",
      "weights=-260579.453125\n",
      "\n",
      "epoch 271:\n",
      "loss=71.15873109726678\n",
      "weights=-260579.875\n",
      "\n",
      "epoch 272:\n",
      "loss=71.15969458080474\n",
      "weights=-260581.109375\n",
      "\n",
      "epoch 273:\n",
      "loss=71.64186032613118\n",
      "weights=-260577.21875\n",
      "\n",
      "epoch 274:\n",
      "loss=70.69560904729934\n",
      "weights=-260580.015625\n",
      "\n",
      "epoch 275:\n",
      "loss=71.20003573099773\n",
      "weights=-260581.828125\n",
      "\n",
      "epoch 276:\n",
      "loss=71.1149282909575\n",
      "weights=-260578.921875\n",
      "\n",
      "epoch 277:\n",
      "loss=70.91166732424782\n",
      "weights=-260576.421875\n",
      "\n",
      "epoch 278:\n",
      "loss=70.98894291832333\n",
      "weights=-260579.671875\n",
      "\n",
      "epoch 279:\n",
      "loss=71.09640216827393\n",
      "weights=-260581.1875\n",
      "\n",
      "epoch 280:\n",
      "loss=70.58357720147995\n",
      "weights=-260579.609375\n",
      "\n",
      "epoch 281:\n",
      "loss=71.35233942667644\n",
      "weights=-260578.5625\n",
      "\n",
      "epoch 282:\n",
      "loss=70.73622585478283\n",
      "weights=-260580.859375\n",
      "\n",
      "epoch 283:\n",
      "loss=70.83532460530598\n",
      "weights=-260581.015625\n",
      "\n",
      "epoch 284:\n",
      "loss=70.58470544360932\n",
      "weights=-260582.453125\n",
      "\n",
      "epoch 285:\n",
      "loss=70.8838309333438\n",
      "weights=-260583.84375\n",
      "\n",
      "epoch 286:\n",
      "loss=70.52028637840634\n",
      "weights=-260585.109375\n",
      "\n",
      "epoch 287:\n",
      "loss=70.56736482892718\n",
      "weights=-260580.4375\n",
      "\n",
      "epoch 288:\n",
      "loss=70.51779710678827\n",
      "weights=-260583.671875\n",
      "\n",
      "epoch 289:\n",
      "loss=70.3213438306536\n",
      "weights=-260585.0625\n",
      "\n",
      "epoch 290:\n",
      "loss=70.45602371579125\n",
      "weights=-260583.71875\n",
      "\n",
      "epoch 291:\n",
      "loss=70.69715181986491\n",
      "weights=-260577.484375\n",
      "\n",
      "epoch 292:\n",
      "loss=70.66449401492164\n",
      "weights=-260571.890625\n",
      "\n",
      "epoch 293:\n",
      "loss=70.72060058230446\n",
      "weights=-260577.640625\n",
      "\n",
      "epoch 294:\n",
      "loss=70.66551480974469\n",
      "weights=-260575.078125\n",
      "\n",
      "epoch 295:\n",
      "loss=70.33177630106609\n",
      "weights=-260576.390625\n",
      "\n",
      "epoch 296:\n",
      "loss=70.67673174540202\n",
      "weights=-260575.625\n",
      "\n",
      "epoch 297:\n",
      "loss=70.45216224307106\n",
      "weights=-260575.265625\n",
      "\n",
      "epoch 298:\n",
      "loss=70.55193601335797\n",
      "weights=-260576.640625\n",
      "\n",
      "epoch 299:\n",
      "loss=69.90718024117606\n",
      "weights=-260574.765625\n",
      "\n",
      "epoch 300:\n",
      "loss=70.32544789995465\n",
      "weights=-260572.640625\n",
      "\n",
      "Decay learning rate\n",
      "epoch 301:\n",
      "loss=69.90309515453521\n",
      "weights=-260572.640625\n",
      "\n",
      "epoch 302:\n",
      "loss=70.37080719357445\n",
      "weights=-260572.625\n",
      "\n",
      "epoch 303:\n",
      "loss=70.16410328093029\n",
      "weights=-260572.40625\n",
      "\n",
      "epoch 304:\n",
      "loss=69.8319746653239\n",
      "weights=-260572.53125\n",
      "\n",
      "epoch 305:\n",
      "loss=70.23232759748187\n",
      "weights=-260572.0625\n",
      "\n",
      "epoch 306:\n",
      "loss=69.99177705673944\n",
      "weights=-260572.53125\n",
      "\n",
      "epoch 307:\n",
      "loss=70.79160045442127\n",
      "weights=-260572.125\n",
      "\n",
      "epoch 308:\n",
      "loss=70.16081056140717\n",
      "weights=-260572.703125\n",
      "\n",
      "epoch 309:\n",
      "loss=69.7099054427374\n",
      "weights=-260572.46875\n",
      "\n",
      "epoch 310:\n",
      "loss=69.72605042230515\n",
      "weights=-260572.875\n",
      "\n",
      "epoch 311:\n",
      "loss=70.48250861394973\n",
      "weights=-260572.71875\n",
      "\n",
      "epoch 312:\n",
      "loss=70.30971263703846\n",
      "weights=-260572.953125\n",
      "\n",
      "epoch 313:\n",
      "loss=70.4363025483631\n",
      "weights=-260572.28125\n",
      "\n",
      "epoch 314:\n",
      "loss=70.57676560538155\n",
      "weights=-260572.25\n",
      "\n",
      "epoch 315:\n",
      "loss=70.04726877666656\n",
      "weights=-260572.203125\n",
      "\n",
      "epoch 316:\n",
      "loss=70.20803996494838\n",
      "weights=-260572.203125\n",
      "\n",
      "epoch 317:\n",
      "loss=70.01576123918805\n",
      "weights=-260572.078125\n",
      "\n",
      "epoch 318:\n",
      "loss=70.12583678109306\n",
      "weights=-260571.421875\n",
      "\n",
      "epoch 319:\n",
      "loss=70.35077703566779\n",
      "weights=-260571.375\n",
      "\n",
      "epoch 320:\n",
      "loss=70.40308007739839\n",
      "weights=-260571.796875\n",
      "\n",
      "epoch 321:\n",
      "loss=69.72517613002232\n",
      "weights=-260571.265625\n",
      "\n",
      "epoch 322:\n",
      "loss=70.39069965907505\n",
      "weights=-260571.90625\n",
      "\n",
      "epoch 323:\n",
      "loss=69.73044050307502\n",
      "weights=-260572.203125\n",
      "\n",
      "epoch 324:\n",
      "loss=70.17464846656436\n",
      "weights=-260571.6875\n",
      "\n",
      "epoch 325:\n",
      "loss=70.40829849243164\n",
      "weights=-260571.828125\n",
      "\n",
      "epoch 326:\n",
      "loss=70.25888806297665\n",
      "weights=-260571.078125\n",
      "\n",
      "epoch 327:\n",
      "loss=69.8996768224807\n",
      "weights=-260571.3125\n",
      "\n",
      "epoch 328:\n",
      "loss=69.48583584740048\n",
      "weights=-260571.25\n",
      "\n",
      "epoch 329:\n",
      "loss=70.1772195725214\n",
      "weights=-260570.578125\n",
      "\n",
      "epoch 330:\n",
      "loss=69.58654903230213\n",
      "weights=-260570.25\n",
      "\n",
      "epoch 331:\n",
      "loss=70.04716546194894\n",
      "weights=-260569.78125\n",
      "\n",
      "epoch 332:\n",
      "loss=70.22272364298503\n",
      "weights=-260569.453125\n",
      "\n",
      "epoch 333:\n",
      "loss=69.70982692355201\n",
      "weights=-260569.1875\n",
      "\n",
      "epoch 334:\n",
      "loss=70.27940804617745\n",
      "weights=-260568.609375\n",
      "\n",
      "epoch 335:\n",
      "loss=69.79801559448242\n",
      "weights=-260569.25\n",
      "\n",
      "epoch 336:\n",
      "loss=70.0848145257859\n",
      "weights=-260568.4375\n",
      "\n",
      "epoch 337:\n",
      "loss=69.88685344514393\n",
      "weights=-260568.296875\n",
      "\n",
      "epoch 338:\n",
      "loss=69.59288251967658\n",
      "weights=-260568.171875\n",
      "\n",
      "epoch 339:\n",
      "loss=70.33842177618118\n",
      "weights=-260567.875\n",
      "\n",
      "epoch 340:\n",
      "loss=70.05438005356561\n",
      "weights=-260567.6875\n",
      "\n",
      "epoch 341:\n",
      "loss=69.78215898786273\n",
      "weights=-260568.0625\n",
      "\n",
      "epoch 342:\n",
      "loss=69.76066598438081\n",
      "weights=-260567.984375\n",
      "\n",
      "epoch 343:\n",
      "loss=70.4692032223656\n",
      "weights=-260567.484375\n",
      "\n",
      "epoch 344:\n",
      "loss=69.8753862835112\n",
      "weights=-260567.59375\n",
      "\n",
      "epoch 345:\n",
      "loss=70.12316340491886\n",
      "weights=-260567.5625\n",
      "\n",
      "epoch 346:\n",
      "loss=69.83798826308478\n",
      "weights=-260568.015625\n",
      "\n",
      "epoch 347:\n",
      "loss=70.03645233880906\n",
      "weights=-260568.125\n",
      "\n",
      "epoch 348:\n",
      "loss=69.6326413835798\n",
      "weights=-260567.578125\n",
      "\n",
      "epoch 349:\n",
      "loss=69.69830967131115\n",
      "weights=-260567.890625\n",
      "\n",
      "epoch 350:\n",
      "loss=70.17646598815918\n",
      "weights=-260567.765625\n",
      "\n",
      "epoch 351:\n",
      "loss=70.33548182532901\n",
      "weights=-260567.6875\n",
      "\n",
      "epoch 352:\n",
      "loss=69.87010401771182\n",
      "weights=-260567.6875\n",
      "\n",
      "epoch 353:\n",
      "loss=69.77119491213844\n",
      "weights=-260567.03125\n",
      "\n",
      "epoch 354:\n",
      "loss=70.17712020874023\n",
      "weights=-260566.546875\n",
      "\n",
      "epoch 355:\n",
      "loss=69.49379421415783\n",
      "weights=-260566.21875\n",
      "\n",
      "epoch 356:\n",
      "loss=70.19735272725423\n",
      "weights=-260567.125\n",
      "\n",
      "epoch 357:\n",
      "loss=69.6222088223412\n",
      "weights=-260566.5625\n",
      "\n",
      "epoch 358:\n",
      "loss=69.69943164643787\n",
      "weights=-260566.84375\n",
      "\n",
      "epoch 359:\n",
      "loss=69.85338338216145\n",
      "weights=-260567.203125\n",
      "\n",
      "epoch 360:\n",
      "loss=69.61969066801525\n",
      "weights=-260566.90625\n",
      "\n",
      "epoch 361:\n",
      "loss=69.58425222124372\n",
      "weights=-260566.90625\n",
      "\n",
      "epoch 362:\n",
      "loss=70.04893039521717\n",
      "weights=-260566.734375\n",
      "\n",
      "epoch 363:\n",
      "loss=69.97265379769462\n",
      "weights=-260566.46875\n",
      "\n",
      "epoch 364:\n",
      "loss=69.89300755092076\n",
      "weights=-260566.140625\n",
      "\n",
      "epoch 365:\n",
      "loss=69.4285516284761\n",
      "weights=-260565.984375\n",
      "\n",
      "epoch 366:\n",
      "loss=70.23951203482491\n",
      "weights=-260565.578125\n",
      "\n",
      "epoch 367:\n",
      "loss=69.97090557643345\n",
      "weights=-260565.53125\n",
      "\n",
      "epoch 368:\n",
      "loss=70.23786190577916\n",
      "weights=-260565.375\n",
      "\n",
      "epoch 369:\n",
      "loss=69.72594115847633\n",
      "weights=-260565.375\n",
      "\n",
      "epoch 370:\n",
      "loss=69.5864945366269\n",
      "weights=-260565.65625\n",
      "\n",
      "epoch 371:\n",
      "loss=69.6607400803339\n",
      "weights=-260565.25\n",
      "\n",
      "epoch 372:\n",
      "loss=69.5835565839495\n",
      "weights=-260565.765625\n",
      "\n",
      "epoch 373:\n",
      "loss=70.15868205115909\n",
      "weights=-260564.625\n",
      "\n",
      "epoch 374:\n",
      "loss=69.63201000576927\n",
      "weights=-260565.1875\n",
      "\n",
      "epoch 375:\n",
      "loss=69.51771327427456\n",
      "weights=-260565.4375\n",
      "\n",
      "epoch 376:\n",
      "loss=69.53993497576032\n",
      "weights=-260565.09375\n",
      "\n",
      "epoch 377:\n",
      "loss=69.82402601696197\n",
      "weights=-260563.84375\n",
      "\n",
      "epoch 378:\n",
      "loss=69.64207313174293\n",
      "weights=-260563.5625\n",
      "\n",
      "epoch 379:\n",
      "loss=69.80959342774891\n",
      "weights=-260563.40625\n",
      "\n",
      "epoch 380:\n",
      "loss=69.80238732837495\n",
      "weights=-260563.296875\n",
      "\n",
      "epoch 381:\n",
      "loss=70.01528858003162\n",
      "weights=-260563.734375\n",
      "\n",
      "epoch 382:\n",
      "loss=70.01609012058803\n",
      "weights=-260563.46875\n",
      "\n",
      "epoch 383:\n",
      "loss=69.70132255554199\n",
      "weights=-260563.734375\n",
      "\n",
      "epoch 384:\n",
      "loss=70.27403386433919\n",
      "weights=-260563.765625\n",
      "\n",
      "epoch 385:\n",
      "loss=69.35505412873768\n",
      "weights=-260563.75\n",
      "\n",
      "epoch 386:\n",
      "loss=69.73795409429641\n",
      "weights=-260564.703125\n",
      "\n",
      "epoch 387:\n",
      "loss=69.46877034505208\n",
      "weights=-260564.4375\n",
      "\n",
      "epoch 388:\n",
      "loss=69.53284645080566\n",
      "weights=-260563.890625\n",
      "\n",
      "epoch 389:\n",
      "loss=69.57230704171317\n",
      "weights=-260564.25\n",
      "\n",
      "epoch 390:\n",
      "loss=69.74943342662993\n",
      "weights=-260564.3125\n",
      "\n",
      "epoch 391:\n",
      "loss=69.76052111671085\n",
      "weights=-260564.265625\n",
      "\n",
      "epoch 392:\n",
      "loss=69.84527478899274\n",
      "weights=-260563.828125\n",
      "\n",
      "epoch 393:\n",
      "loss=70.48674438113258\n",
      "weights=-260563.625\n",
      "\n",
      "epoch 394:\n",
      "loss=69.83253251938592\n",
      "weights=-260563.40625\n",
      "\n",
      "epoch 395:\n",
      "loss=69.80619703020368\n",
      "weights=-260563.28125\n",
      "\n",
      "epoch 396:\n",
      "loss=69.51854505993072\n",
      "weights=-260563.671875\n",
      "\n",
      "epoch 397:\n",
      "loss=70.34850611005511\n",
      "weights=-260563.359375\n",
      "\n",
      "epoch 398:\n",
      "loss=69.79791441417876\n",
      "weights=-260563.484375\n",
      "\n",
      "epoch 399:\n",
      "loss=70.02103514898391\n",
      "weights=-260562.84375\n",
      "\n",
      "epoch 400:\n",
      "loss=69.60376839410691\n",
      "weights=-260562.671875\n",
      "\n",
      "Decay learning rate\n",
      "epoch 401:\n",
      "loss=69.78432964143299\n",
      "weights=-260562.609375\n",
      "\n",
      "epoch 402:\n",
      "loss=69.56878980000813\n",
      "weights=-260562.640625\n",
      "\n",
      "epoch 403:\n",
      "loss=69.68151601155598\n",
      "weights=-260562.75\n",
      "\n",
      "epoch 404:\n",
      "loss=69.7065862928118\n",
      "weights=-260562.59375\n",
      "\n",
      "epoch 405:\n",
      "loss=70.27660878499348\n",
      "weights=-260562.703125\n",
      "\n",
      "epoch 406:\n",
      "loss=69.58323387872605\n",
      "weights=-260562.59375\n",
      "\n",
      "epoch 407:\n",
      "loss=69.77273523239862\n",
      "weights=-260562.75\n",
      "\n",
      "epoch 408:\n",
      "loss=69.53610692705426\n",
      "weights=-260562.609375\n",
      "\n",
      "epoch 409:\n",
      "loss=70.09877913338798\n",
      "weights=-260562.71875\n",
      "\n",
      "epoch 410:\n",
      "loss=69.50823266165597\n",
      "weights=-260562.703125\n",
      "\n",
      "epoch 411:\n",
      "loss=69.80339459010533\n",
      "weights=-260562.71875\n",
      "\n",
      "epoch 412:\n",
      "loss=70.00989468892415\n",
      "weights=-260562.609375\n",
      "\n",
      "epoch 413:\n",
      "loss=69.84430340358189\n",
      "weights=-260562.59375\n",
      "\n",
      "epoch 414:\n",
      "loss=69.84099097478958\n",
      "weights=-260562.53125\n",
      "\n",
      "epoch 415:\n",
      "loss=69.6737805321103\n",
      "weights=-260562.5\n",
      "\n",
      "epoch 416:\n",
      "loss=70.00146393548874\n",
      "weights=-260562.578125\n",
      "\n",
      "epoch 417:\n",
      "loss=69.90385237194243\n",
      "weights=-260562.546875\n",
      "\n",
      "epoch 418:\n",
      "loss=70.08189519246419\n",
      "weights=-260562.5625\n",
      "\n",
      "epoch 419:\n",
      "loss=69.46127846127465\n",
      "weights=-260562.65625\n",
      "\n",
      "epoch 420:\n",
      "loss=69.68974776495071\n",
      "weights=-260562.53125\n",
      "\n",
      "epoch 421:\n",
      "loss=69.66766502743675\n",
      "weights=-260562.65625\n",
      "\n",
      "epoch 422:\n",
      "loss=70.04834647405715\n",
      "weights=-260562.625\n",
      "\n",
      "epoch 423:\n",
      "loss=69.76995813278924\n",
      "weights=-260562.53125\n",
      "\n",
      "epoch 424:\n",
      "loss=70.10854693821499\n",
      "weights=-260562.484375\n",
      "\n",
      "epoch 425:\n",
      "loss=69.41512716384162\n",
      "weights=-260562.5\n",
      "\n",
      "epoch 426:\n",
      "loss=69.67226137433734\n",
      "weights=-260562.40625\n",
      "\n",
      "epoch 427:\n",
      "loss=69.31626710437592\n",
      "weights=-260562.359375\n",
      "\n",
      "epoch 428:\n",
      "loss=70.04998888288226\n",
      "weights=-260562.625\n",
      "\n",
      "epoch 429:\n",
      "loss=69.54276747930618\n",
      "weights=-260562.453125\n",
      "\n",
      "epoch 430:\n",
      "loss=69.93218985058013\n",
      "weights=-260562.484375\n",
      "\n",
      "epoch 431:\n",
      "loss=69.99413408551898\n",
      "weights=-260562.578125\n",
      "\n",
      "epoch 432:\n",
      "loss=69.78716287158784\n",
      "weights=-260562.5625\n",
      "\n",
      "epoch 433:\n",
      "loss=69.55520039512997\n",
      "weights=-260562.484375\n",
      "\n",
      "epoch 434:\n",
      "loss=69.62913367861793\n",
      "weights=-260562.484375\n",
      "\n",
      "epoch 435:\n",
      "loss=69.91813369024368\n",
      "weights=-260562.4375\n",
      "\n",
      "epoch 436:\n",
      "loss=69.51817221868606\n",
      "weights=-260562.5625\n",
      "\n",
      "epoch 437:\n",
      "loss=69.99465497334798\n",
      "weights=-260562.484375\n",
      "\n",
      "epoch 438:\n",
      "loss=69.3233402797154\n",
      "weights=-260562.484375\n",
      "\n",
      "epoch 439:\n",
      "loss=69.54379472278413\n",
      "weights=-260562.5625\n",
      "\n",
      "epoch 440:\n",
      "loss=69.65499732607887\n",
      "weights=-260562.484375\n",
      "\n",
      "epoch 441:\n",
      "loss=69.665010088966\n",
      "weights=-260562.484375\n",
      "\n",
      "epoch 442:\n",
      "loss=69.30160431634812\n",
      "weights=-260562.46875\n",
      "\n",
      "epoch 443:\n",
      "loss=69.79830437614804\n",
      "weights=-260562.421875\n",
      "\n",
      "epoch 444:\n",
      "loss=69.36141123090472\n",
      "weights=-260562.421875\n",
      "\n",
      "epoch 445:\n",
      "loss=69.40466817220052\n",
      "weights=-260562.5\n",
      "\n",
      "epoch 446:\n",
      "loss=69.57544962565105\n",
      "weights=-260562.359375\n",
      "\n",
      "epoch 447:\n",
      "loss=69.64924567086356\n",
      "weights=-260562.484375\n",
      "\n",
      "epoch 448:\n",
      "loss=69.78459757850284\n",
      "weights=-260562.390625\n",
      "\n",
      "epoch 449:\n",
      "loss=69.7038456144787\n",
      "weights=-260562.40625\n",
      "\n",
      "epoch 450:\n",
      "loss=69.58676864987328\n",
      "weights=-260562.34375\n",
      "\n",
      "epoch 451:\n",
      "loss=69.74867847987584\n",
      "weights=-260562.328125\n",
      "\n",
      "epoch 452:\n",
      "loss=69.41495177859352\n",
      "weights=-260562.421875\n",
      "\n",
      "epoch 453:\n",
      "loss=69.50773030235653\n",
      "weights=-260562.296875\n",
      "\n",
      "epoch 454:\n",
      "loss=70.07104374113537\n",
      "weights=-260562.375\n",
      "\n",
      "epoch 455:\n",
      "loss=69.50082860674176\n",
      "weights=-260562.25\n",
      "\n",
      "epoch 456:\n",
      "loss=69.77360407511394\n",
      "weights=-260562.28125\n",
      "\n",
      "epoch 457:\n",
      "loss=69.5260518391927\n",
      "weights=-260562.328125\n",
      "\n",
      "epoch 458:\n",
      "loss=69.94622076125373\n",
      "weights=-260562.375\n",
      "\n",
      "epoch 459:\n",
      "loss=69.76833724975586\n",
      "weights=-260562.21875\n",
      "\n",
      "epoch 460:\n",
      "loss=69.85818990071614\n",
      "weights=-260562.28125\n",
      "\n",
      "epoch 461:\n",
      "loss=69.4373722076416\n",
      "weights=-260562.203125\n",
      "\n",
      "epoch 462:\n",
      "loss=69.38128825596401\n",
      "weights=-260562.140625\n",
      "\n",
      "epoch 463:\n",
      "loss=69.78625406537738\n",
      "weights=-260562.078125\n",
      "\n",
      "epoch 464:\n",
      "loss=69.84748840332031\n",
      "weights=-260561.953125\n",
      "\n",
      "epoch 465:\n",
      "loss=69.70335669744583\n",
      "weights=-260562.203125\n",
      "\n",
      "epoch 466:\n",
      "loss=69.92626589820499\n",
      "weights=-260562.21875\n",
      "\n",
      "epoch 467:\n",
      "loss=69.67815680730911\n",
      "weights=-260562.265625\n",
      "\n",
      "epoch 468:\n",
      "loss=69.73580887204125\n",
      "weights=-260562.28125\n",
      "\n",
      "epoch 469:\n",
      "loss=69.33230000450497\n",
      "weights=-260562.171875\n",
      "\n",
      "epoch 470:\n",
      "loss=69.57125927153088\n",
      "weights=-260562.15625\n",
      "\n",
      "epoch 471:\n",
      "loss=69.83919970194499\n",
      "weights=-260562.171875\n",
      "\n",
      "epoch 472:\n",
      "loss=69.52067484174457\n",
      "weights=-260562.234375\n",
      "\n",
      "epoch 473:\n",
      "loss=69.48512249901181\n",
      "weights=-260562.21875\n",
      "\n",
      "epoch 474:\n",
      "loss=69.85459727332706\n",
      "weights=-260562.171875\n",
      "\n",
      "epoch 475:\n",
      "loss=69.29397401355561\n",
      "weights=-260562.140625\n",
      "\n",
      "epoch 476:\n",
      "loss=69.87728037152972\n",
      "weights=-260562.09375\n",
      "\n",
      "epoch 477:\n",
      "loss=69.92196900503976\n",
      "weights=-260562.03125\n",
      "\n",
      "epoch 478:\n",
      "loss=70.29056194850376\n",
      "weights=-260562.109375\n",
      "\n",
      "epoch 479:\n",
      "loss=70.20649728320893\n",
      "weights=-260562.15625\n",
      "\n",
      "epoch 480:\n",
      "loss=69.58658704303559\n",
      "weights=-260562.171875\n",
      "\n",
      "epoch 481:\n",
      "loss=70.27494630359467\n",
      "weights=-260562.125\n",
      "\n",
      "epoch 482:\n",
      "loss=69.52012071155366\n",
      "weights=-260562.203125\n",
      "\n",
      "epoch 483:\n",
      "loss=69.67195801507859\n",
      "weights=-260562.09375\n",
      "\n",
      "epoch 484:\n",
      "loss=69.21613002958752\n",
      "weights=-260562.078125\n",
      "\n",
      "epoch 485:\n",
      "loss=69.35623005458287\n",
      "weights=-260562.03125\n",
      "\n",
      "epoch 486:\n",
      "loss=69.65660712832496\n",
      "weights=-260562.03125\n",
      "\n",
      "epoch 487:\n",
      "loss=69.66602125621978\n",
      "weights=-260562.09375\n",
      "\n",
      "epoch 488:\n",
      "loss=69.77649661472866\n",
      "weights=-260562.09375\n",
      "\n",
      "epoch 489:\n",
      "loss=69.80533527192615\n",
      "weights=-260562.109375\n",
      "\n",
      "epoch 490:\n",
      "loss=69.41790580749512\n",
      "weights=-260562.0625\n",
      "\n",
      "epoch 491:\n",
      "loss=69.92316509428478\n",
      "weights=-260562.203125\n",
      "\n",
      "epoch 492:\n",
      "loss=69.88614436558315\n",
      "weights=-260562.046875\n",
      "\n",
      "epoch 493:\n",
      "loss=69.90638514927456\n",
      "weights=-260562.03125\n",
      "\n",
      "epoch 494:\n",
      "loss=69.89432017008464\n",
      "weights=-260562.046875\n",
      "\n",
      "epoch 495:\n",
      "loss=69.87288402375721\n",
      "weights=-260562.15625\n",
      "\n",
      "epoch 496:\n",
      "loss=69.95369130089169\n",
      "weights=-260561.9375\n",
      "\n",
      "epoch 497:\n",
      "loss=69.3753136226109\n",
      "weights=-260561.953125\n",
      "\n",
      "epoch 498:\n",
      "loss=69.92150933401925\n",
      "weights=-260562.078125\n",
      "\n",
      "epoch 499:\n",
      "loss=70.02151689075288\n",
      "weights=-260562.015625\n",
      "\n",
      "Training time 6611.590837240219\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "# Train AE\n",
    "with open('deap_topomap.pkl', 'rb') as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "target_model = finetuning.Finetuning(model=\"AE\", n_freezelayers=0, seed=seed)  \n",
    "sum_params = 0\n",
    "for param in target_model.model.parameters():\n",
    "    sum_params += torch.sum(param)\n",
    "print(sum_params)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adamax(target_model.model.parameters(), lr = lr)\n",
    "target_model.train(dataset, log_dir='./runs/deap/AEAdamax/epoch500/seed'+str(seed), optimizer=optimizer, lr=lr, epochs=500, early_stopping_eps=0.0001, lr_decay_nepoch=100)\n",
    "torch.save(target_model.model.state_dict(), 'AE_deap_topomap_adamax_epoch500_seed'+str(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d6717b3-348e-492c-a14b-6b9922f14fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deap_topomap.pkl', 'rb') as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "subjects = list(np.unique(dataset.sessions))\n",
    "train_acc, acc, evs = per_subject_cv(dataset, subjects, lda, n_jobs=64, checkpoint='/home/jovyan/deep-eeg/notebooks/AE_deap_topomap_adamax_epoch500_seed9')\n",
    "with open(f\"result_finetune_deap_topomap_AE.pkl\", \"wb\") as fout:\n",
    "    pickle.dump({\"acc\": acc, \n",
    "                 \"train_acc\": train_acc,\n",
    "                 \"ev\": evs}, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ddb3f-680a-46d7-b6db-1dd995fcbe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overview(train_acc, acc, evs, dataset, dataset_clip=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b36494-1ffa-433a-b658-fe1e651038be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-74918.9375, grad_fn=<AddBackward0>)\n",
      "Loading weights from checkpoint\n",
      "tensor(-187311.6719, grad_fn=<AddBackward0>)\n",
      "tensor(252., device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd8d11d2a10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_model = finetuning.Finetuning(model=\"AE\", n_freezelayers=0)  \n",
    "sample, label = target_model._input_augment_fn(dataset[28][0], dataset[0][1])\n",
    "sample = torch.squeeze(sample).transpose(0,2)\n",
    "# sample -= sample.min(1, keepdim=True)[0]\n",
    "# sample /= sample.max(1, keepdim=True)[0]\n",
    "print(sample.max())\n",
    "plt.imshow(np.uint16(sample.numpy(force=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc5dcd-0b37-4f5c-a94a-3c07d5c7c684",
   "metadata": {},
   "source": [
    "# Deap Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ac6c3-23f8-4178-8799-303d67b7256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "# Train AE\n",
    "with open(\"deap_spectra.pkl\", \"rb\") as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "target_model = finetuning.Finetuning(model=\"AE\", n_freezelayers=0, seed=seed)  \n",
    "sum_params = 0\n",
    "for param in target_model.model.parameters():\n",
    "    sum_params += torch.sum(param)\n",
    "print(sum_params)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adamax(target_model.model.parameters(), lr = lr)\n",
    "target_model.train(dataset, log_dir='./runs/deap/specto/AEAdamax/epoch500/seed'+str(seed), optimizer=optimizer, lr=lr, epochs=500, early_stopping_eps=0.0001, lr_decay_nepoch=100, checkpoint_path='./checkpoints/deap/specto/AEAdamax', save_every=10)\n",
    "torch.save(target_model.model.state_dict(), 'AE_deap_specto_adamax_epoch500_seed'+str(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffff7175-7b01-49b7-832b-6021b3a41851",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deap_spectra.pkl', 'rb') as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "target_model = finetuning.Finetuning(model=\"AE\", n_freezelayers=0, model_params={'checkpoint': '/home/jovyan/deep-eeg/notebooks/checkpoints/deap/specto/AEAdamax/seed9/epoch_90'})  \n",
    "sample, label = target_model._input_augment_fn(dataset[18][0], dataset[0][1])\n",
    "print(sample.shape)\n",
    "recons = target_model.model(sample.unsqueeze(0))\n",
    "# recons -= recons.min(1, keepdim=True)[0]\n",
    "# recons /= recons.max(1, keepdim=True)[0]\n",
    "recons = torch.squeeze(recons).transpose(0,2)\n",
    "# recons.shape\n",
    "print(recons.max())\n",
    "plt.imshow(recons.numpy(force=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d189245-2181-46ac-b640-3f08fff23bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = finetuning.Finetuning(model=\"AE\", n_freezelayers=0, model_params={'checkpoint': '/home/jovyan/deep-eeg/notebooks/AE_mahnob_topomap_adamax_epoch350'})  \n",
    "sample, label = target_model._input_augment_fn(dataset[18][0], dataset[0][1])\n",
    "sample = torch.squeeze(sample).transpose(0,2)\n",
    "# sample -= sample.min(1, keepdim=True)[0]\n",
    "# sample /= sample.max(1, keepdim=True)[0]\n",
    "print(sample.max())\n",
    "print(sample.min())\n",
    "plt.imshow(sample.numpy(force=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15ef03-3790-478c-babe-4c3f3316bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deap_spectra.pkl', 'rb') as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "subjects = list(np.unique(dataset.sessions))\n",
    "train_acc, acc, evs = per_subject_cv(dataset, subjects, lda, n_jobs=64, checkpoint='/home/jovyan/deep-eeg/notebooks/checkpoints/deap/specto/AEAdamax/seed9/epoch_90')\n",
    "with open(f\"result_finetune_deap_specto_AE.pkl\", \"wb\") as fout:\n",
    "    pickle.dump({\"acc\": acc, \n",
    "                 \"train_acc\": train_acc,\n",
    "                 \"ev\": evs}, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b57237-8840-4478-a6ce-55fa426e165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"result_finetune_deap_specto_AE.pkl\", \"rb\") as fin:\n",
    "    result = pickle.load(fin)\n",
    "    acc = result['acc']\n",
    "    train_acc = result[\"train_acc\"]\n",
    "    evs = result[\"ev\"]\n",
    "print(acc.shape, train_acc.shape, evs.shape)\n",
    "plot_overview(train_acc, acc, evs, dataset, dataset_clip=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039e6ed-5196-4854-b257-cc0523608319",
   "metadata": {},
   "source": [
    "# Dreamer Topomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97edfef-491b-4140-8ae6-c94e0647b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 14\n",
    "ds = 'dreamer'\n",
    "feat = 'topomap'\n",
    "# Train AE\n",
    "with open('dreamer_topomap.pkl', 'rb') as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "target_model = finetuning.Finetuning(model=\"AE\", n_freezelayers=0, seed=seed)  \n",
    "sum_params = 0\n",
    "for param in target_model.model.parameters():\n",
    "    sum_params += torch.sum(param)\n",
    "print(sum_params)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adamax(target_model.model.parameters(), lr = lr)\n",
    "target_model.train(dataset, log_dir=f'./runs/{ds}/{feat}/AEAdamax/seed{seed}', optimizer=optimizer, lr=lr, epochs=500, early_stopping_eps=0.000001, lr_decay_nepoch=100, checkpoint_path=f'./checkpoints/{ds}/{feat}/AEAdamax/seed{seed}', save_every=10)\n",
    "# torch.save(target_model.model.state_dict(), 'AE_dreamer_specto_adamax_epoch500_seed'+str(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc6c4e-33aa-40e4-9a87-fe53211d7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dreamer_topomap.pkl', 'rb') as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "subjects = list(np.unique(dataset.sessions))\n",
    "train_acc, acc, evs = per_subject_cv(dataset, subjects, lda, n_jobs=64, checkpoint='/home/jovyan/deep-eeg/notebooks/checkpoints/dreamer/topomap/AEAdamax/seed12/epoch_490')\n",
    "with open(f\"result_finetune_dreamer_topomap_AE_seed12_epoch490.pkl\", \"wb\") as fout:\n",
    "    pickle.dump({\"acc\": acc, \n",
    "                 \"train_acc\": train_acc,\n",
    "                 \"ev\": evs}, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4451a-b030-401f-aebd-a4f67b5613b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overview(train_acc, acc, evs, dataset, dataset_clip=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72fd04-e7ee-46de-aa90-37a5402d7d02",
   "metadata": {},
   "source": [
    "# Dreamer Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e7d6c-b5a6-4808-afa5-44a1828939f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dreamer.DreamerDataset(\n",
    "    x_params={\n",
    "        \"feature\": \"SpectrogramImg\",\n",
    "    },\n",
    "    sessions=None,\n",
    "    y_mode='split',\n",
    "    y_keys=['arousal'],\n",
    "    seed=49,\n",
    "    balanced=False,\n",
    ")\n",
    "subjects = list(np.unique(dataset.sessions))\n",
    "with open('dreamer_spectrogram.pkl', 'wb') as fout:\n",
    "    pickle.dump({'dataset': dataset}, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731aa419-2ae2-48fb-bc77-fc60d1172465",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = finetuning.Finetuning(model=\"AE\", n_freezelayers=0, model_params={'checkpoint': '/home/jovyan/deep-eeg/notebooks/AE_mahnob_topomap_adamax_epoch350'})  \n",
    "sample, label = target_model._input_augment_fn(dataset[8][0], dataset[0][1])\n",
    "print(sample.shape)\n",
    "sample = torch.squeeze(sample).transpose(0,2)\n",
    "# sample -= sample.min(1, keepdim=True)[0]\n",
    "# sample /= sample.max(1, keepdim=True)[0]\n",
    "print(sample.max())\n",
    "print(sample.min())\n",
    "plt.imshow(sample.numpy(force=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c5820-2815-48bc-a307-1c653fc2caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dreamer_spectrogram.pkl', 'rb') as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "target_model = finetuning.Finetuning(model=\"AE\", n_freezelayers=0) \n",
    "sample, label = target_model._input_augment_fn(dataset[8][0], dataset[0][1])\n",
    "print(sample.shape)\n",
    "recons = target_model.model(sample.unsqueeze(0))\n",
    "# recons -= recons.min(1, keepdim=True)[0]\n",
    "# recons /= recons.max(1, keepdim=True)[0]\n",
    "recons = torch.squeeze(recons).transpose(0,2)\n",
    "# recons.shape\n",
    "print(recons.max())\n",
    "plt.imshow(recons.numpy(force=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb8c4a2-1d32-4f12-8aa0-d05e2b6f7c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 9\n",
    "# Train AE\n",
    "with open('dreamer_spectrogram.pkl', 'rb') as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "target_model = finetuning.Finetuning(model=\"AE\", n_freezelayers=0, seed=seed, model_params={'checkpoint': '/home/jovyan/deep-eeg/notebooks/checkpoints/dreamer/specto/AEAdamax/seed9/epoch_320'})  \n",
    "sum_params = 0\n",
    "for param in target_model.model.parameters():\n",
    "    sum_params += torch.sum(param)\n",
    "print(sum_params)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adamax(target_model.model.parameters(), lr = lr)\n",
    "target_model.train(dataset, log_dir='/home/jovyan/deep-eeg/notebooks/runs/dreamer/specto/AEAdamax/seed'+str(seed), optimizer=optimizer, lr=lr, start_from=321, epochs=300, early_stopping_eps=0.000001, lr_decay_nepoch=100, checkpoint_path='./checkpoints/dreamer/specto/AEAdamax/seed'+str(seed), save_every=10)\n",
    "# torch.save(target_model.model.state_dict(), 'AE_dreamer_specto_adamax_epoch800_seed'+str(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6780140-76ea-4fe9-8770-6f46e0be7cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dreamer_spectrogram.pkl', 'rb') as fin:\n",
    "    dataset = pickle.load(fin)\n",
    "    dataset = dataset['dataset']\n",
    "subjects = list(np.unique(dataset.sessions))\n",
    "torch.cuda.empty_cache()\n",
    "train_acc, acc, evs = per_subject_cv(dataset, subjects, lda, n_jobs=64, checkpoint='/home/jovyan/deep-eeg/notebooks/checkpoints/dreamer/specto/AEAdamax/seed9/epoch_320')\n",
    "with open(f\"result_finetune_dreamer_specto_AE.pkl\", \"wb\") as fout:\n",
    "    pickle.dump({\"acc\": acc, \n",
    "                 \"train_acc\": train_acc,\n",
    "                 \"ev\": evs}, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf0969-f63c-4838-bfbf-64fbbcc667ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc.shape, train_acc.shape, evs.shape)\n",
    "plot_overview(train_acc, acc, evs, dataset, dataset_clip=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
